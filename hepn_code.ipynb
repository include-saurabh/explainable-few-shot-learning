{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hierarchical Explainable Prototypical Network (HEPN) on CUB-200-2011\n",
        "\n",
        "Integrates learned part prototypes into the few-shot meta-learning process\n",
        "for enhanced explainability. Manual Grad-CAM implementation."
      ],
      "metadata": {
        "id": "EKz6JdTveIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KIhrphsN7ORk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4_f1noD6zEd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2 # For resizing CAM and colormap\n",
        "import copy\n",
        "import json # For split saving/loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Configuration ---\n",
        "# -------------------------------------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Dataset params\n",
        "# Adapt DATA_DIR to use BASE_PATH\n",
        "BASE_PATH = '/content/drive/MyDrive' # Update with your path\n",
        "DATA_DIR = os.path.join(BASE_PATH, 'CUB_200_2011')\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
        "IMAGE_SIZE = 224\n",
        "SPLIT_FILE = os.path.join(BASE_PATH, \"cub_meta_split.json\") # Use BASE_PATH\n",
        "FORCE_RESPLIT = False # Set True to ignore saved split\n",
        "\n",
        "# Episode params (Adjust as needed)\n",
        "N_WAY = 3\n",
        "K_SHOT = 5\n",
        "N_QUERY = 10\n",
        "N_TRAIN_EPISODES = 5000 # Needs sufficient episodes to learn parts\n",
        "N_TEST_EPISODES = 600\n",
        "\n",
        "# --- HEPN Model Params ---\n",
        "EMBEDDING_DIM_GLOBAL = 256  # Dimension for global embeddings (Level 1)\n",
        "EMBEDDING_DIM_PATCH = 512   # Dimension for patch embeddings (Level 2 - ResNet18 output)\n",
        "NUM_PART_PROTOTYPES = 5    # Number of learnable generic part prototypes (Hyperparameter)\n",
        "PART_PROJECTION_INTERVAL = 500 # How often (in episodes) to project part prototypes\n",
        "PROJECTION_BATCH_SIZE = 128 # Batch size for projection dataset processing\n",
        "\n",
        "# Encoder Params\n",
        "PRETRAINED = True\n",
        "FREEZE_UNTIL_LAYER = \"layer3\" # Or None, layer1, layer2 etc.\n",
        "DROPOUT_RATE = 0.5\n",
        "TARGET_LAYER_NAME = 'encoder.layer4' # Target layer for manual Grad-CAM\n",
        "\n",
        "# Training params\n",
        "LR_BACKBONE = 1e-5\n",
        "LR_HEAD_GLOBAL = 1e-4\n",
        "LR_PART_PROTOTYPES = 1e-4 # Learning rate for the part prototypes themselves\n",
        "LR_COMBINER = 1e-4      # Learning rate for combination layer/params\n",
        "WEIGHT_DECAY = 5e-4\n",
        "LABEL_SMOOTHING = 0.1\n",
        "GRADIENT_CLIP_NORM = 1.0\n",
        "\n",
        "# --- Loss Weights (Crucial Hyperparameters) ---\n",
        "LAMBDA_CLST = 0.1         # Weight for Part Prototype Cluster Loss\n",
        "LAMBDA_DIVERSITY = 0.05   # Weight for Part Prototype Diversity Loss\n",
        "LAMBDA_L1 = 0.00          # Weight for L1 penalty on part activations (encourages sparsity)\n",
        "LAMBDA_PART_SIM = 0.5     # Weight for combining part similarity score into final logits\n",
        "\n",
        "# Visualization Params\n",
        "# PATCH_SIZE_VIS = 28 # Note: Actual patch size depends on encoder stride"
      ],
      "metadata": {
        "id": "1e5n6voheVSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -------------------------------------\n",
        "# --- CUB Data Handling (Robust Version) ---\n",
        "# -------------------------------------\n",
        "\n",
        "def parse_cub_metadata(data_dir):\n",
        "    \"\"\"Reads CUB metadata files into pandas DataFrames.\"\"\"\n",
        "    images_path = os.path.join(data_dir, 'images.txt')\n",
        "    labels_path = os.path.join(data_dir, 'image_class_labels.txt')\n",
        "    split_path = os.path.join(data_dir, 'train_test_split.txt')\n",
        "    bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n",
        "    classes_path = os.path.join(data_dir, 'classes.txt')\n",
        "\n",
        "    required_files = [images_path, labels_path, split_path, bbox_path, classes_path]\n",
        "    for f_path in required_files:\n",
        "        if not os.path.exists(f_path):\n",
        "            raise FileNotFoundError(f\"Metadata file not found: {f_path}.\")\n",
        "\n",
        "    df_images = pd.read_csv(images_path, sep=' ', names=['img_id', 'filepath'])\n",
        "    df_labels = pd.read_csv(labels_path, sep=' ', names=['img_id', 'class_id'])\n",
        "    df_split = pd.read_csv(split_path, sep=' ', names=['img_id', 'is_training'])\n",
        "    df_bboxes = pd.read_csv(bbox_path, sep=' ', names=['img_id', 'x', 'y', 'width', 'height'])\n",
        "    df_classes = pd.read_csv(classes_path, sep=' ', names=['class_id', 'class_name'])\n",
        "\n",
        "    df_labels['class_id'] = df_labels['class_id'] - 1 # 0-based\n",
        "    df = df_images.merge(df_labels, on='img_id')\n",
        "    df = df.merge(df_split, on='img_id')\n",
        "    df = df.merge(df_bboxes, on='img_id')\n",
        "    df['full_path'] = df['filepath'].apply(lambda x: os.path.join(IMAGES_DIR, x))\n",
        "    class_id_to_name = df_classes.set_index('class_id')['class_name'].to_dict()\n",
        "    class_id_to_name = {(k - 1): v for k, v in class_id_to_name.items()} # Adjust keys to 0-based\n",
        "    print(f\"Parsed metadata for {len(df)} total image entries across {df['class_id'].nunique()} original classes.\")\n",
        "    return df, class_id_to_name\n",
        "\n",
        "\n",
        "class CubDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for CUB-200-2011 with cropping.\"\"\"\n",
        "    def __init__(self, df_subset, transform=None):\n",
        "        self.df = df_subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_info = self.df.iloc[idx]\n",
        "        img_path = img_info['full_path']\n",
        "        label = img_info['subset_class_id'] # Use 0-based subset ID\n",
        "        bbox = (img_info['x'], img_info['y'], img_info['width'], img_info['height'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found {img_path}. Skipping.\")\n",
        "            return None, -1 # Return None image, handle in dataloader or loop\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, -1\n",
        "\n",
        "        x, y, w, h = bbox\n",
        "        left, upper = int(np.floor(x)), int(np.floor(y))\n",
        "        right, lower = int(np.ceil(x + w)), int(np.ceil(y + h))\n",
        "        img_width, img_height = image.size\n",
        "        left, upper = max(0, left), max(0, upper)\n",
        "        right, lower = min(img_width, right), min(img_height, lower)\n",
        "\n",
        "        if right > left and lower > upper:\n",
        "             image = image.crop((left, upper, right, lower))\n",
        "        # else: Optional: print warning about invalid crop using full image\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Check if transform failed (e.g., returned None)\n",
        "        if image is None:\n",
        "            print(f\"Warning: Transform returned None for image {img_path}. Skipping.\")\n",
        "            return None, -1\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def prepare_cub_data_splits(data_dir, n_meta_train_ratio=0.7, split_save_path=\"cub_meta_split.json\", force_resplit=False):\n",
        "    \"\"\"\n",
        "    Loads CUB data, detects available classes, prepares meta-splits\n",
        "    (loading existing split if available, otherwise creating and saving),\n",
        "    creates data lists for samplers, and returns a Dataset for meta-train.\n",
        "    \"\"\"\n",
        "    images_dir_local = os.path.join(data_dir, 'images')\n",
        "    if not os.path.isdir(images_dir_local):\n",
        "         raise FileNotFoundError(f\"Images directory not found: {images_dir_local}\")\n",
        "\n",
        "    df_all, class_id_to_name_map = parse_cub_metadata(data_dir)\n",
        "\n",
        "    # --- Detect Available Classes ---\n",
        "    available_folders = [d for d in os.listdir(images_dir_local) if os.path.isdir(os.path.join(images_dir_local, d))]\n",
        "    available_original_class_ids = []\n",
        "    for folder_name in available_folders:\n",
        "        try:\n",
        "            class_num_str = folder_name.split('.')[0]\n",
        "            original_id_one_based = int(class_num_str)\n",
        "            available_original_class_ids.append(original_id_one_based - 1) # Store 0-based original ID\n",
        "        except ValueError:\n",
        "             print(f\"Skipping folder '{folder_name}' - cannot parse class number.\")\n",
        "             continue\n",
        "    available_original_class_ids = sorted(list(set(available_original_class_ids)))\n",
        "\n",
        "    if not available_original_class_ids:\n",
        "        raise ValueError(\"No valid class folders found or parsed.\")\n",
        "\n",
        "    N_CLASSES_TOTAL_DETECTED = len(available_original_class_ids)\n",
        "    print(f\"\\nDetected {N_CLASSES_TOTAL_DETECTED} classes based on folder names.\")\n",
        "    if N_CLASSES_TOTAL_DETECTED > 0:\n",
        "        print(f\"Sample detected original class IDs (0-based): {available_original_class_ids[:5]}...\")\n",
        "\n",
        "    # --- Load or Create Meta-Split ---\n",
        "    meta_train_subset_indices = None\n",
        "    meta_test_subset_indices = None\n",
        "\n",
        "    if os.path.exists(split_save_path) and not force_resplit:\n",
        "        print(f\"Loading existing class split from: {split_save_path}\")\n",
        "        try:\n",
        "            with open(split_save_path, 'r') as f:\n",
        "                split_data = json.load(f)\n",
        "            if ('meta_train_indices' in split_data and 'meta_test_indices' in split_data and\n",
        "                isinstance(split_data['meta_train_indices'], list) and\n",
        "                isinstance(split_data['meta_test_indices'], list)):\n",
        "                loaded_train_indices = split_data['meta_train_indices']\n",
        "                loaded_test_indices = split_data['meta_test_indices']\n",
        "                all_loaded_indices = set(loaded_train_indices + loaded_test_indices)\n",
        "                available_set = set(available_original_class_ids)\n",
        "                invalid_indices = all_loaded_indices - available_set\n",
        "\n",
        "                if not invalid_indices:\n",
        "                    if not set(loaded_train_indices).intersection(set(loaded_test_indices)):\n",
        "                        meta_train_subset_indices = loaded_train_indices\n",
        "                        meta_test_subset_indices = loaded_test_indices\n",
        "                        print(\"Successfully loaded and validated existing split (using original class IDs).\")\n",
        "                    else: print(\"Warning: Loaded split has overlapping train/test indices. Will create new split.\")\n",
        "                else: print(f\"Warning: Loaded split contains original class IDs ({invalid_indices}) not present in currently detected classes ({available_set}). Will create new split.\")\n",
        "            else: print(f\"Warning: Invalid format in {split_save_path}. Will create new split.\")\n",
        "        except Exception as e: print(f\"Error loading split file {split_save_path}: {e}. Will create new split.\")\n",
        "\n",
        "    if meta_train_subset_indices is None or meta_test_subset_indices is None:\n",
        "        print(\"Creating new meta-train/test class split...\")\n",
        "        n_meta_train_actual = int(len(available_original_class_ids) * n_meta_train_ratio)\n",
        "        n_meta_test_actual = len(available_original_class_ids) - n_meta_train_actual\n",
        "        if n_meta_train_actual <= 0 or n_meta_test_actual <= 0:\n",
        "            raise ValueError(f\"Cannot split {len(available_original_class_ids)} detected classes into train/test with ratio {n_meta_train_ratio}. Need more classes or adjust ratio.\")\n",
        "\n",
        "        shuffled_available_ids = random.sample(available_original_class_ids, len(available_original_class_ids))\n",
        "        meta_train_subset_indices = sorted(shuffled_available_ids[:n_meta_train_actual]) # Store ORIGINAL IDs\n",
        "        meta_test_subset_indices = sorted(shuffled_available_ids[n_meta_train_actual:]) # Store ORIGINAL IDs\n",
        "\n",
        "        try:\n",
        "            split_data_to_save = {'meta_train_indices': meta_train_subset_indices, 'meta_test_indices': meta_test_subset_indices,\n",
        "                                  'comment': f'Split of {len(available_original_class_ids)} detected classes based on original CUB IDs.'}\n",
        "            with open(split_save_path, 'w') as f: json.dump(split_data_to_save, f, indent=4)\n",
        "            print(f\"Saved new class split (using original IDs) to: {split_save_path}\")\n",
        "        except Exception as e: print(f\"Error saving new split file {split_save_path}: {e}\")\n",
        "\n",
        "    n_meta_train_actual = len(meta_train_subset_indices)\n",
        "    n_meta_test_actual = len(meta_test_subset_indices)\n",
        "    print(f\"Using finalized split: {n_meta_train_actual} meta-train classes, {n_meta_test_actual} meta-test classes (based on original CUB IDs).\")\n",
        "\n",
        "    # Filter main dataframe based on DETECTED available original IDs\n",
        "    df_subset = df_all[df_all['class_id'].isin(available_original_class_ids)].copy()\n",
        "\n",
        "    # Create map from original ID to NEW, CONTIGUOUS 0-based subset ID (includes ALL detected)\n",
        "    all_detected_subset_map = {orig_id: new_id for new_id, orig_id in enumerate(available_original_class_ids)}\n",
        "    df_subset.loc[:, 'subset_class_id'] = df_subset['class_id'].map(all_detected_subset_map)\n",
        "\n",
        "    # Get the subset_class_ids for the chosen meta-train/test original IDs\n",
        "    meta_train_subset_class_ids = sorted([all_detected_subset_map[orig_id] for orig_id in meta_train_subset_indices])\n",
        "    meta_test_subset_class_ids = sorted([all_detected_subset_map[orig_id] for orig_id in meta_test_subset_indices])\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Adjust global N_WAY if needed\n",
        "    global N_WAY\n",
        "    max_way_train = len(meta_train_subset_class_ids)\n",
        "    max_way_test = len(meta_test_subset_class_ids)\n",
        "    original_N_WAY = N_WAY\n",
        "    updated_n_way = False\n",
        "    if N_WAY > max_way_train:\n",
        "        print(f\"WARNING: N_WAY ({N_WAY}) > available meta-train classes ({max_way_train}). Reducing N_WAY to {max_way_train}.\")\n",
        "        N_WAY = max_way_train\n",
        "        updated_n_way = True\n",
        "    if N_WAY > max_way_test:\n",
        "        # Check if N_WAY was already reduced\n",
        "        current_n_way_for_test_check = N_WAY if updated_n_way else original_N_WAY\n",
        "        print(f\"WARNING: N_WAY ({current_n_way_for_test_check}) > available meta-test classes ({max_way_test}). Reducing N_WAY to {max_way_test}.\")\n",
        "        N_WAY = max_way_test\n",
        "        updated_n_way = True\n",
        "    if N_WAY <= 1:\n",
        "        print(f\"CRITICAL WARNING: N_WAY reduced to {N_WAY}. Few-shot learning might not be meaningful.\")\n",
        "\n",
        "    # Separate CUB's original train/test split *within* our meta-split classes (using SUBSET CLASS IDs)\n",
        "    df_meta_train_pool = df_subset[df_subset['subset_class_id'].isin(meta_train_subset_class_ids)]\n",
        "    df_meta_test_pool = df_subset[df_subset['subset_class_id'].isin(meta_test_subset_class_ids)]\n",
        "    df_meta_train_from_cub_train = df_meta_train_pool[df_meta_train_pool['is_training'] == 1]\n",
        "    df_meta_test_from_cub_test = df_meta_test_pool[df_meta_test_pool['is_training'] == 0]\n",
        "\n",
        "    # --- Helper to load data for samplers ---\n",
        "    def load_data_for_sampler(target_subset_class_ids, df_source, split_name):\n",
        "        data = [] # List of (local_sampler_class_id, [image_tensor, ...])\n",
        "        class_map_for_sampler = {subset_id: i for i, subset_id in enumerate(target_subset_class_ids)}\n",
        "\n",
        "        pbar = tqdm(target_subset_class_ids, desc=f\"Processing Meta-{split_name} Classes for Sampler\")\n",
        "        for subset_class_id in pbar:\n",
        "            local_label = class_map_for_sampler[subset_class_id]\n",
        "            class_df = df_source[df_source['subset_class_id'] == subset_class_id]\n",
        "            if not class_df.empty:\n",
        "                temp_class_df = class_df.copy()\n",
        "                temp_class_df.loc[:, 'subset_class_id'] = local_label # Overwrite with 0-based local label\n",
        "                temp_dataset = CubDataset(temp_class_df, transform=transform)\n",
        "                images, labels = [], []\n",
        "                for i in range(len(temp_dataset)):\n",
        "                    img, lbl = temp_dataset[i]\n",
        "                    if img is not None: # Skip if image loading/transform failed\n",
        "                        images.append(img)\n",
        "                        labels.append(lbl)\n",
        "                if images:\n",
        "                    data.append((local_label, images))\n",
        "            pbar.set_postfix({\"Class\": subset_class_id, \"Images\": len(images) if images else 0})\n",
        "\n",
        "        data.sort(key=lambda x: x[0])\n",
        "        min_samples_needed = K_SHOT + N_QUERY\n",
        "        valid_classes_count = sum(1 for _, imgs in data if len(imgs) >= min_samples_needed)\n",
        "        num_classes_in_sampler = len(data)\n",
        "        if valid_classes_count < num_classes_in_sampler: print(f\"*** WARNING ({split_name} Sampler): {num_classes_in_sampler - valid_classes_count}/{num_classes_in_sampler} classes have < {min_samples_needed} samples. Replacement needed.\")\n",
        "        if valid_classes_count < N_WAY: print(f\"*** CRITICAL WARNING ({split_name} Sampler): Only {valid_classes_count} classes have enough samples. N_WAY ({N_WAY}) might be impossible without replacement.\")\n",
        "        return data\n",
        "\n",
        "    print(\"\\nBuilding meta-train data for sampler (from CUB train split)...\")\n",
        "    meta_train_data = load_data_for_sampler(meta_train_subset_class_ids, df_meta_train_from_cub_train, \"Train\")\n",
        "    print(\"\\nBuilding meta-test data for sampler (from CUB test split)...\")\n",
        "    meta_test_data = load_data_for_sampler(meta_test_subset_class_ids, df_meta_test_from_cub_test, \"Test\")\n",
        "\n",
        "    # --- Create the Full Meta-Train Dataset for Projection ---\n",
        "    print(\"\\nCreating full meta-train dataset for projection...\")\n",
        "    meta_train_full_dataset = CubDataset(df_meta_train_from_cub_train, transform=transform)\n",
        "    print(f\"Full meta-train dataset size (for projection): {len(meta_train_full_dataset)} images\")\n",
        "\n",
        "    print(f\"\\nFinal classes available for meta-train sampler: {len(meta_train_data)}\")\n",
        "    print(f\"Final classes available for meta-test sampler: {len(meta_test_data)}\")\n",
        "\n",
        "    if not meta_train_data or not meta_test_data or len(meta_train_full_dataset) == 0:\n",
        "        print(\"\\n*** ERROR: Empty meta_train_data, meta_test_data, or meta_train_full_dataset. Check data. ***\")\n",
        "        return None, None, None\n",
        "\n",
        "    return meta_train_data, meta_test_data, meta_train_full_dataset\n",
        "\n",
        "# --- Episode Sampler (Unchanged) ---\n",
        "class EpisodeSampler:\n",
        "    \"\"\"Samples episodes for N-way K-shot learning.\"\"\"\n",
        "    def __init__(self, meta_data, n_way, k_shot, n_query):\n",
        "        self.meta_data = meta_data\n",
        "        if not meta_data: raise ValueError(\"Meta data cannot be empty\")\n",
        "        self.num_classes = len(meta_data)\n",
        "        if n_way <= 0 or n_way > self.num_classes:\n",
        "            raise ValueError(f\"Invalid n_way ({n_way}) for {self.num_classes} available classes.\")\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.n_query = n_query\n",
        "        # Check minimum samples required per class\n",
        "        self.min_samples_needed = k_shot + n_query\n",
        "        self.classes_with_enough_samples = [\n",
        "            i for i, (_, images) in enumerate(meta_data) if len(images) >= self.min_samples_needed\n",
        "        ]\n",
        "        self.classes_requiring_replacement = [\n",
        "            i for i, (_, images) in enumerate(meta_data) if len(images) < self.min_samples_needed\n",
        "        ]\n",
        "        if len(self.classes_with_enough_samples) < self.n_way:\n",
        "             print(f\"WARNING (Sampler Init): Only {len(self.classes_with_enough_samples)} classes have >= {self.min_samples_needed} samples. \"\n",
        "                   f\"N_WAY ({self.n_way}) requires sampling *with replacement* from some classes even for class selection, \"\n",
        "                   f\"or relying heavily on image replacement within selected classes.\")\n",
        "             # Consider alternative: raise ValueError if len(self.classes_with_enough_samples) == 0\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        support_imgs, support_lbls, query_imgs, query_lbls = [], [], [], []\n",
        "\n",
        "        # Prioritize sampling classes with enough samples if possible\n",
        "        available_indices = list(range(self.num_classes))\n",
        "        try:\n",
        "            # Sample class indices. If fewer than N_WAY have enough samples,\n",
        "            # replacement=True might be necessary for the class selection itself,\n",
        "            # though ideally we avoid sampling the same class multiple times in one episode.\n",
        "            # A safer strategy is to sample from all available, even if some require image replacement later.\n",
        "            sampled_class_indices = random.sample(available_indices, self.n_way)\n",
        "        except ValueError:\n",
        "             print(f\"Error: Cannot sample {self.n_way} distinct classes from {self.num_classes} available.\")\n",
        "             # Fallback: sample with replacement if absolutely necessary (not ideal for few-shot)\n",
        "             # sampled_class_indices = random.choices(available_indices, k=self.n_way)\n",
        "             # Or simply raise the error:\n",
        "             raise ValueError(f\"Not enough unique classes ({self.num_classes}) available to sample N_WAY={self.n_way}.\")\n",
        "\n",
        "\n",
        "        for local_lbl, class_idx in enumerate(sampled_class_indices):\n",
        "            _, images = self.meta_data[class_idx]\n",
        "            n_available = len(images)\n",
        "            n_needed = self.k_shot + self.n_query\n",
        "\n",
        "            if n_available == 0:\n",
        "                print(f\"Warning: Class index {class_idx} (local label {local_lbl}) has 0 images. Skipping.\")\n",
        "                # This case should ideally be prevented by checks in data loading.\n",
        "                # If it happens, the episode might be smaller than intended.\n",
        "                # A more robust approach might be to resample the class or episode.\n",
        "                continue # Skip this class for this episode\n",
        "\n",
        "            use_replacement = n_available < n_needed\n",
        "            if use_replacement:\n",
        "                # print(f\"Note: Using replacement for class index {class_idx} (local {local_lbl}) - needed {n_needed}, got {n_available}\")\n",
        "                indices = random.choices(range(n_available), k=n_needed)\n",
        "            else:\n",
        "                indices = random.sample(range(n_available), n_needed)\n",
        "\n",
        "            support_imgs.extend([images[i] for i in indices[:self.k_shot]])\n",
        "            support_lbls.extend([local_lbl] * self.k_shot)\n",
        "            query_imgs.extend([images[i] for i in indices[self.k_shot:]])\n",
        "            query_lbls.extend([local_lbl] * self.n_query)\n",
        "\n",
        "        # Check if any images were actually added before stacking\n",
        "        if not support_imgs or not query_imgs:\n",
        "             # This could happen if all selected classes had 0 images\n",
        "             raise ValueError(\"Failed to gather any support or query images for the episode.\")\n",
        "\n",
        "\n",
        "        support_imgs = torch.stack(support_imgs)\n",
        "        support_lbls = torch.LongTensor(support_lbls)\n",
        "        query_imgs = torch.stack(query_imgs)\n",
        "        query_lbls = torch.LongTensor(query_lbls)\n",
        "\n",
        "        # Shuffle query set\n",
        "        perm = torch.randperm(len(query_lbls))\n",
        "        query_imgs, query_lbls = query_imgs[perm], query_lbls[perm]\n",
        "\n",
        "        return support_imgs, support_lbls, query_imgs, query_lbls"
      ],
      "metadata": {
        "id": "vg3P9AE0em7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- HEPN Model Architecture ---\n",
        "# -------------------------------------\n",
        "\n",
        "class EncoderHEPN(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for HEPN. Outputs both patch-level conv features and global embedding.\n",
        "    Based on ResNet18 with freezing and dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim_global=EMBEDDING_DIM_GLOBAL,\n",
        "                 patch_feature_dim=EMBEDDING_DIM_PATCH, # Should match ResNet layer4 output\n",
        "                 pretrained=True, freeze_until=None, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "\n",
        "        # Backbone Conv Layers\n",
        "        self.stem = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4 # Output feature map for patches\n",
        "\n",
        "        # Keep individual layers accessible, also group them\n",
        "        self.conv_feature_extractor_layers = [self.stem, self.layer1, self.layer2, self.layer3, self.layer4]\n",
        "\n",
        "        if freeze_until:\n",
        "            self._freeze_layers(freeze_until)\n",
        "\n",
        "        # Head for Global Embedding (Level 1)\n",
        "        resnet_out_dim = resnet.fc.in_features # 512 for ResNet18\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.global_dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.global_embedding_layer = nn.Linear(resnet_out_dim, embedding_dim_global)\n",
        "\n",
        "        self.global_embedding_head = nn.Sequential(\n",
        "            self.global_pool,\n",
        "            self.flatten,\n",
        "            self.global_dropout,\n",
        "            self.global_embedding_layer\n",
        "        )\n",
        "\n",
        "        # Patch feature dimension is output of last conv layer (layer4)\n",
        "        self.patch_feature_dim = resnet_out_dim\n",
        "        if self.patch_feature_dim != patch_feature_dim:\n",
        "             print(f\"Info: Requested patch_feature_dim {patch_feature_dim} differs \"\n",
        "                   f\"from ResNet output {self.patch_feature_dim}. Using {self.patch_feature_dim}.\")\n",
        "             # No override needed, just use the actual dim from ResNet\n",
        "\n",
        "        self.total_stride = 32 # For ResNet18 (2*2*2*2 * 2 from stem maxpool)\n",
        "\n",
        "    def _freeze_layers(self, freeze_until):\n",
        "        print(f\"Freezing ResNet layers up to and including: {freeze_until}\")\n",
        "        layers_to_freeze = []\n",
        "        if freeze_until == \"stem\": layers_to_freeze = [self.stem]\n",
        "        elif freeze_until == \"layer1\": layers_to_freeze = [self.stem, self.layer1]\n",
        "        elif freeze_until == \"layer2\": layers_to_freeze = [self.stem, self.layer1, self.layer2]\n",
        "        elif freeze_until == \"layer3\": layers_to_freeze = [self.stem, self.layer1, self.layer2, self.layer3]\n",
        "        elif freeze_until == \"layer4\": layers_to_freeze = [self.stem, self.layer1, self.layer2, self.layer3, self.layer4]\n",
        "        elif freeze_until is None or freeze_until.lower() == \"none\":\n",
        "            print(\"No layers frozen.\")\n",
        "            return\n",
        "        else:\n",
        "            print(f\"Warning: Unknown freeze_until value '{freeze_until}'. No layers frozen.\")\n",
        "            return\n",
        "\n",
        "        for layer in layers_to_freeze:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = False\n",
        "        print(f\"Finished freezing {len(layers_to_freeze)} layer groups.\")\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through convolutional layers sequentially\n",
        "        conv_features = x\n",
        "        for layer in self.conv_feature_extractor_layers:\n",
        "            conv_features = layer(conv_features)\n",
        "        # Now conv_features is the output of layer4: [B, D_patch, H, W]\n",
        "\n",
        "        global_embedding = self.global_embedding_head(conv_features) # [B, E_global]\n",
        "        return conv_features, global_embedding\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        backbone_params = []\n",
        "        head_params = [] # Global head params\n",
        "        # Iterate through named parameters to identify head vs backbone\n",
        "        for name, param in self.named_parameters():\n",
        "            if not param.requires_grad:\n",
        "                continue # Skip frozen layers\n",
        "            # Check if param belongs to the global embedding head\n",
        "            if name.startswith('global_embedding_layer.') or \\\n",
        "               name.startswith('global_dropout.') or \\\n",
        "               name.startswith('flatten.') or \\\n",
        "               name.startswith('global_pool.'): # Check module names\n",
        "                head_params.append(param)\n",
        "            else:\n",
        "                # Assume remaining trainable params are part of the conv backbone\n",
        "                backbone_params.append(param)\n",
        "\n",
        "        print(f\"Encoder Param Groups: {len(backbone_params)} backbone, {len(head_params)} global_head\")\n",
        "        return backbone_params, head_params\n",
        "\n",
        "\n",
        "class PartPrototypeLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Learns generic part prototypes and calculates part activation profiles.\n",
        "    Includes prototype projection functionality.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_parts=NUM_PART_PROTOTYPES, patch_feature_dim=EMBEDDING_DIM_PATCH):\n",
        "        super().__init__()\n",
        "        self.num_parts = num_parts\n",
        "        self.patch_feature_dim = patch_feature_dim\n",
        "\n",
        "        # Learnable part prototypes\n",
        "        self.part_prototypes = nn.Parameter(torch.randn(self.num_parts, self.patch_feature_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # Store img_idx (in dataset), h_idx, w_idx of the patch projected onto\n",
        "        self.register_buffer('projected_patch_locations', torch.full((self.num_parts, 3), -1, dtype=torch.long)) # Store img_idx, h, w\n",
        "        self.register_buffer('projected_image_indices', torch.full((self.num_parts,), -1, dtype=torch.long)) # Redundant? Keep maybe.\n",
        "\n",
        "\n",
        "    def _calculate_similarity(self, patches, prototypes):\n",
        "        \"\"\" Calculates patch-prototype similarity (negative squared L2 distance) \"\"\"\n",
        "        # Patches: [N_Patches, D]\n",
        "        # Prototypes: [M_Prototypes, D]\n",
        "        # Ensure consistent dtype for cdist\n",
        "        patches_f = patches.float()\n",
        "        prototypes_f = prototypes.float()\n",
        "        distances_sq = torch.cdist(patches_f, prototypes_f)**2 # [N_Patches, M_Prototypes]\n",
        "        similarity = -distances_sq\n",
        "        return similarity\n",
        "\n",
        "    def calculate_query_activation(self, query_conv_features):\n",
        "        \"\"\" Calculates the part activation vector for a query image. \"\"\"\n",
        "        # query_conv_features: [B, D, H, W] (Assume B=1 for single query)\n",
        "        B, D, H, W = query_conv_features.shape\n",
        "        if B != 1: print(f\"Warning: calculate_query_activation called with B={B}\")\n",
        "        N_PATCHES_Q = H * W\n",
        "        if N_PATCHES_Q <= 0: return torch.zeros(self.num_parts, device=query_conv_features.device)\n",
        "\n",
        "        query_patches = query_conv_features.permute(0, 2, 3, 1).reshape(N_PATCHES_Q, D) # [HW, D]\n",
        "        sim_matrix = self._calculate_similarity(query_patches, self.part_prototypes) # [HW, M]\n",
        "\n",
        "        # Max similarity for each prototype across all query patches\n",
        "        activation_q, _ = torch.max(sim_matrix, dim=0) # [M]\n",
        "        return activation_q\n",
        "\n",
        "    def calculate_class_profile(self, support_conv_features_list):\n",
        "        \"\"\" Calculates the average part activation profile for a class's support set. \"\"\"\n",
        "        if isinstance(support_conv_features_list, list):\n",
        "            if not support_conv_features_list: return torch.zeros(self.num_parts, device=self.part_prototypes.device)\n",
        "            support_conv_features = torch.stack(support_conv_features_list, dim=0) # [K, D, H, W]\n",
        "        else:\n",
        "             support_conv_features = support_conv_features_list # Assume already stacked tensor\n",
        "\n",
        "        K, D, H, W = support_conv_features.shape\n",
        "        N_PATCHES_S = H * W\n",
        "        if K == 0 or N_PATCHES_S <= 0: return torch.zeros(self.num_parts, device=support_conv_features.device)\n",
        "\n",
        "        support_patches = support_conv_features.permute(0, 2, 3, 1).reshape(K * N_PATCHES_S, D) # [K*HW, D]\n",
        "        sim_matrix_s = self._calculate_similarity(support_patches, self.part_prototypes) # [K*HW, M]\n",
        "        sim_matrix_s = sim_matrix_s.view(K, N_PATCHES_S, self.num_parts) # [K, HW, M]\n",
        "\n",
        "        # Max similarity per image in the support set\n",
        "        max_sim_per_image, _ = torch.max(sim_matrix_s, dim=1) # [K, M]\n",
        "\n",
        "        # Average activation profile across the support set\n",
        "        class_profile = torch.mean(max_sim_per_image, dim=0) # [M]\n",
        "        return class_profile\n",
        "\n",
        "    def calculate_part_similarity_score(self, activation_q, class_profile):\n",
        "        \"\"\" Calculates similarity between query activation and class profile (e.g., Cosine Sim). \"\"\"\n",
        "        return F.cosine_similarity(activation_q.unsqueeze(0), class_profile.unsqueeze(0), eps=1e-6).squeeze()\n",
        "\n",
        "    # --- Losses ---\n",
        "    def calculate_cluster_loss(self, conv_features):\n",
        "        \"\"\" Encourages patches to be close to *some* part prototype. \"\"\"\n",
        "        B, D, H, W = conv_features.shape\n",
        "        N_PATCHES = H * W\n",
        "        if B == 0 or N_PATCHES <= 0: return torch.tensor(0.0, device=conv_features.device)\n",
        "\n",
        "        patches = conv_features.permute(0, 2, 3, 1).reshape(B * N_PATCHES, D)\n",
        "        dist_matrix_sq = torch.cdist(patches.float(), self.part_prototypes.float())**2 # [B*HW, M]\n",
        "        min_dist_sq, _ = torch.min(dist_matrix_sq, dim=1) # [B*HW]\n",
        "        cluster_loss = torch.mean(min_dist_sq)\n",
        "        return cluster_loss\n",
        "\n",
        "    def calculate_diversity_loss(self):\n",
        "        \"\"\" Encourage part prototypes to be distinct (maximize avg distance). \"\"\"\n",
        "        proto_dist_sq = torch.cdist(self.part_prototypes.float(), self.part_prototypes.float())**2 # [M, M]\n",
        "        n = self.num_parts\n",
        "        if n <= 1: return torch.tensor(0.0, device=self.part_prototypes.device)\n",
        "        # Sum upper triangle (excluding diagonal)\n",
        "        sum_dist_sq = torch.triu(proto_dist_sq, diagonal=1).sum()\n",
        "        num_pairs = n * (n - 1) / 2\n",
        "        avg_dist_sq = sum_dist_sq / num_pairs if num_pairs > 0 else 1.0\n",
        "        # Penalize inverse of average distance (lower loss = larger avg dist)\n",
        "        diversity_loss = 1.0 / (avg_dist_sq + 1e-6)\n",
        "        return diversity_loss\n",
        "\n",
        "    def calculate_l1_loss(self, activations):\n",
        "         \"\"\" Penalizes dense part activations (if used). \"\"\"\n",
        "         if activations is None or activations.numel() == 0:\n",
        "             return torch.tensor(0.0, device=self.part_prototypes.device)\n",
        "         return torch.mean(torch.abs(activations))\n",
        "\n",
        "    # --- Prototype Projection ---\n",
        "    @torch.no_grad()\n",
        "    def project_part_prototypes(self, dataset_for_proj, encoder_model, device, batch_size=PROJECTION_BATCH_SIZE):\n",
        "        \"\"\"\n",
        "        Finds the patch feature vector in the dataset closest to each prototype\n",
        "        and updates the prototype to be that patch feature vector. Includes refined checks.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Starting Part Prototype Projection (Dataset size: {len(dataset_for_proj)}) ---\")\n",
        "        if len(dataset_for_proj) == 0:\n",
        "            print(\"Warning: Projection dataset is empty. Skipping projection.\")\n",
        "            return\n",
        "\n",
        "        original_encoder_mode = encoder_model.training\n",
        "        encoder_model.eval()\n",
        "\n",
        "        # Filter out None items from dataset BEFORE creating DataLoader\n",
        "        # This requires iterating through the dataset once, could be slow for large datasets\n",
        "        # Alternatively, handle None inside the dataloader loop (collate_fn can help)\n",
        "        valid_indices = [i for i, (img, _) in enumerate(dataset_for_proj) if img is not None]\n",
        "        if len(valid_indices) != len(dataset_for_proj):\n",
        "            print(f\"Warning: Filtering out {len(dataset_for_proj) - len(valid_indices)} None items from projection dataset.\")\n",
        "            dataset_for_proj_filtered = torch.utils.data.Subset(dataset_for_proj, valid_indices)\n",
        "            # Store mapping from filtered index to original index if needed for location tracking\n",
        "            index_map = {new_idx: orig_idx for new_idx, orig_idx in enumerate(valid_indices)}\n",
        "        else:\n",
        "            dataset_for_proj_filtered = dataset_for_proj\n",
        "            index_map = {i: i for i in range(len(dataset_for_proj))} # Identity map\n",
        "\n",
        "        if len(dataset_for_proj_filtered) == 0:\n",
        "             print(\"Warning: Projection dataset empty after filtering None items. Skipping projection.\")\n",
        "             encoder_model.train(original_encoder_mode)\n",
        "             return\n",
        "\n",
        "        pin_memory_flag = True if device.type == 'cuda' else False\n",
        "        num_workers = min(4, os.cpu_count() // 2 if os.cpu_count() else 1) if pin_memory_flag else 0\n",
        "        dataloader = DataLoader(dataset_for_proj_filtered, batch_size=batch_size, shuffle=False,\n",
        "                                num_workers=num_workers, pin_memory=pin_memory_flag)\n",
        "\n",
        "        all_patch_features = []\n",
        "        all_patch_origins = [] # List of (original_dataset_idx, h, w)\n",
        "        current_filtered_idx = 0\n",
        "\n",
        "        pbar = tqdm(dataloader, desc=\"Extracting Features for Projection\", leave=False)\n",
        "        for images, _ in pbar:\n",
        "            if images is None: continue # Should be handled by filtering, but double-check\n",
        "            images = images.to(device, non_blocking=pin_memory_flag)\n",
        "            conv_features, _ = encoder_model(images)\n",
        "            B, D, H, W = conv_features.shape\n",
        "            if H <= 0 or W <= 0: continue\n",
        "\n",
        "            patches = conv_features.permute(0, 2, 3, 1).reshape(B * H * W, D)\n",
        "            all_patch_features.append(patches.cpu()) # Store on CPU first\n",
        "\n",
        "            for b in range(B):\n",
        "                # Map filtered index back to original dataset index\n",
        "                original_dataset_index = index_map[current_filtered_idx + b]\n",
        "                for h in range(H):\n",
        "                    for w in range(W):\n",
        "                        all_patch_origins.append((original_dataset_index, h, w))\n",
        "            current_filtered_idx += B\n",
        "            pbar.set_postfix({\"Patches\": f\"{len(all_patch_origins):,}\"})\n",
        "\n",
        "\n",
        "        if not all_patch_features:\n",
        "            print(\"Warning: No patch features extracted. Skipping projection.\")\n",
        "            encoder_model.train(original_encoder_mode)\n",
        "            return\n",
        "\n",
        "        all_patch_features = torch.cat(all_patch_features, dim=0)\n",
        "        print(f\"Extracted {all_patch_features.shape[0]:,} patch features ({all_patch_features.element_size() * all_patch_features.nelement() / 1024**2:.1f} MB on CPU).\")\n",
        "\n",
        "        # Move to device in chunks if necessary, but try all at once first\n",
        "        try:\n",
        "            all_patch_features = all_patch_features.to(device)\n",
        "            print(f\"Moved all patch features to {device}.\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error moving all patch features to {device}: {e}. Projection might fail or be slow.\")\n",
        "            # Implement chunking here if needed\n",
        "            encoder_model.train(original_encoder_mode)\n",
        "            return # Or proceed with CPU-based projection if memory is an issue\n",
        "\n",
        "        min_dists_sq = torch.full((self.num_parts,), float('inf'), device=device, dtype=torch.float32)\n",
        "        nearest_patch_indices = torch.full((self.num_parts,), -1, dtype=torch.long, device=device)\n",
        "        intended_locations = {} # {proto_idx: (orig_img_idx, h, w)}\n",
        "\n",
        "        print(\"Finding nearest patches for prototypes...\")\n",
        "        # Process prototypes in batches to potentially reduce memory usage during cdist\n",
        "        proto_batch_size = 10 # Adjust based on GPU memory\n",
        "        num_proto_batches = math.ceil(self.num_parts / proto_batch_size)\n",
        "\n",
        "        for i in tqdm(range(num_proto_batches), desc=\"Projecting Prototypes\", leave=False):\n",
        "            start_idx = i * proto_batch_size\n",
        "            end_idx = min((i + 1) * proto_batch_size, self.num_parts)\n",
        "            current_proto_indices_abs = torch.arange(start_idx, end_idx, device='cpu')\n",
        "            current_prototypes = self.part_prototypes[start_idx:end_idx].to(device=device, dtype=torch.float32)\n",
        "\n",
        "            # Calculate distances between all patches and the current batch of prototypes\n",
        "            dist_matrix_sq = torch.cdist(all_patch_features, current_prototypes)**2 # [N_Patches, N_Proto_Batch]\n",
        "\n",
        "            # Find the closest patch for each prototype *in this batch*\n",
        "            batch_min_dists_sq, batch_nearest_patch_indices_in_all = torch.min(dist_matrix_sq, dim=0) # [N_Proto_Batch]\n",
        "\n",
        "            # Update global minimums and corresponding patch indices\n",
        "            min_dists_sq[start_idx:end_idx] = batch_min_dists_sq\n",
        "            nearest_patch_indices[start_idx:end_idx] = batch_nearest_patch_indices_in_all\n",
        "\n",
        "            # Store intended locations for the successfully found patches in this batch\n",
        "            valid_batch_mask = batch_nearest_patch_indices_in_all != -1 # Should usually be true\n",
        "            valid_global_patch_indices = batch_nearest_patch_indices_in_all[valid_batch_mask]\n",
        "            valid_batch_proto_indices_rel = torch.where(valid_batch_mask)[0]\n",
        "            valid_batch_proto_indices_abs = current_proto_indices_abs[valid_batch_proto_indices_rel.cpu()]\n",
        "\n",
        "            batch_origins = [all_patch_origins[idx.item()] for idx in valid_global_patch_indices.cpu()]\n",
        "            for k, abs_proto_idx in enumerate(valid_batch_proto_indices_abs.tolist()):\n",
        "                intended_locations[abs_proto_idx] = batch_origins[k] # Store (orig_img_idx, h, w)\n",
        "\n",
        "        # --- Overall Validation and Update ---\n",
        "        valid_mask = nearest_patch_indices != -1\n",
        "        valid_proto_indices = torch.where(valid_mask)[0] # Indices of prototypes that found a match\n",
        "        valid_nearest_patch_indices = nearest_patch_indices[valid_mask] # Indices into all_patch_features\n",
        "\n",
        "        if len(valid_nearest_patch_indices) == 0:\n",
        "             print(\"Error: No valid nearest patches found for any prototype. Skipping update.\")\n",
        "        else:\n",
        "            print(f\"Projecting {len(valid_proto_indices)}/{self.num_parts} prototypes.\")\n",
        "            updated_prototype_features = all_patch_features[valid_nearest_patch_indices]\n",
        "            # Update the parameters IN-PLACE\n",
        "            self.part_prototypes.data[valid_proto_indices] = updated_prototype_features.to(self.part_prototypes.dtype)\n",
        "\n",
        "            # Update the location buffers only for the projected prototypes\n",
        "            valid_origins = [intended_locations.get(idx.item()) for idx in valid_proto_indices.cpu()]\n",
        "            valid_origins_filtered = [o for o in valid_origins if o is not None] # Filter out None if lookup failed\n",
        "\n",
        "            if len(valid_origins_filtered) != len(valid_proto_indices):\n",
        "                 print(f\"*** WARNING: Mismatch between valid protos ({len(valid_proto_indices)}) and found origins ({len(valid_origins_filtered)}). Check logic.\")\n",
        "                 # Decide how to handle: maybe only update buffers for those with origins?\n",
        "                 # For now, proceed but this indicates a potential issue.\n",
        "\n",
        "            if valid_origins_filtered:\n",
        "                # Create tensors for image index, h, w from the filtered origins\n",
        "                proj_img_indices_valid = torch.tensor([o[0] for o in valid_origins_filtered], dtype=torch.long, device=self.projected_image_indices.device)\n",
        "                proj_locations_valid = torch.tensor([(o[1], o[2]) for o in valid_origins_filtered], dtype=torch.long, device=self.projected_patch_locations.device)\n",
        "\n",
        "                # Map valid_origins_filtered back to the correct valid_proto_indices\n",
        "                # This is tricky if filtering occurred. Let's assume no filtering for simplicity first,\n",
        "                # or update the buffers based on the indices that *did* have origins found.\n",
        "                # Simplest: Assume indices match if no filtering occurred\n",
        "                if len(valid_origins_filtered) == len(valid_proto_indices):\n",
        "                    # Update projected_image_indices buffer\n",
        "                    self.projected_image_indices.data[valid_proto_indices] = proj_img_indices_valid\n",
        "\n",
        "                    # Update projected_patch_locations buffer (img_idx, h, w)\n",
        "                    # First column is img_idx, next two are h, w\n",
        "                    self.projected_patch_locations.data[valid_proto_indices, 0] = proj_img_indices_valid # Store img_idx\n",
        "                    self.projected_patch_locations.data[valid_proto_indices, 1:3] = proj_locations_valid # Store h, w\n",
        "                else:\n",
        "                    print(\"Skipping buffer update due to origin/proto mismatch.\")\n",
        "            else:\n",
        "                 print(\"No valid location origins found. Buffers not updated.\")\n",
        "\n",
        "            # --- Final Sanity Check (Example for Proto 0) ---\n",
        "            check_proto_idx_abs = 0\n",
        "            if check_proto_idx_abs in valid_proto_indices.tolist():\n",
        "                check_patch_idx_abs = nearest_patch_indices[check_proto_idx_abs].item()\n",
        "                final_dist_check = torch.dist(\n",
        "                    self.part_prototypes[check_proto_idx_abs].to(device).float(),\n",
        "                    all_patch_features[check_patch_idx_abs].to(device).float()\n",
        "                ).item()\n",
        "                min_dist_found = min_dists_sq[check_proto_idx_abs].sqrt().item()\n",
        "                intended_img, intended_h, intended_w = intended_locations.get(check_proto_idx_abs, (-1,-1,-1))\n",
        "                buffer_img = self.projected_patch_locations[check_proto_idx_abs, 0].item()\n",
        "                buffer_h = self.projected_patch_locations[check_proto_idx_abs, 1].item()\n",
        "                buffer_w = self.projected_patch_locations[check_proto_idx_abs, 2].item()\n",
        "                print(f\"Projection Check (Proto {check_proto_idx_abs}): Projected.\")\n",
        "                print(f\"  - Final dist: {final_dist_check:.4f} (min found: {min_dist_found:.4f})\")\n",
        "                print(f\"  - Intended Origin (Img, h, w): ({intended_img}, {intended_h}, {intended_w})\")\n",
        "                print(f\"  - Buffer Read (Img, h, w): ({buffer_img}, {buffer_h}, {buffer_w})\")\n",
        "                if buffer_img != intended_img or buffer_h != intended_h or buffer_w != intended_w:\n",
        "                     print(\"  - *** WARNING: Final check shows mismatch between intended and buffer read! ***\")\n",
        "            else:\n",
        "                 print(f\"Projection Check (Proto {check_proto_idx_abs}): Not projected (or index out of bounds).\")\n",
        "\n",
        "\n",
        "        del all_patch_features, dist_matrix_sq # Free memory\n",
        "        if device.type == 'cuda': torch.cuda.empty_cache()\n",
        "\n",
        "        encoder_model.train(original_encoder_mode) # Restore original mode\n",
        "        print(\"--- Finished Part Prototype Projection ---\")\n",
        "\n",
        "\n",
        "class HEPN(nn.Module):\n",
        "    \"\"\" Hierarchical Explainable Prototypical Network \"\"\"\n",
        "    def __init__(self, encoder, num_parts=NUM_PART_PROTOTYPES,\n",
        "                 lambda_part_sim=LAMBDA_PART_SIM, combination_method='add'):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.part_prototype_layer = PartPrototypeLayer(\n",
        "            num_parts=num_parts,\n",
        "            patch_feature_dim=encoder.patch_feature_dim # Use actual dim from encoder\n",
        "        )\n",
        "        self.lambda_part_sim = lambda_part_sim\n",
        "        self.combination_method = combination_method\n",
        "\n",
        "        if self.combination_method == 'mlp':\n",
        "            # Simple MLP: combines global logit and part similarity score\n",
        "            self.combiner_mlp = nn.Sequential(\n",
        "                nn.Linear(2, 16),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(16, 1)\n",
        "            )\n",
        "            print(\"Using MLP combiner.\")\n",
        "        else:\n",
        "            self.combiner_mlp = None\n",
        "            print(\"Using Additive combiner.\")\n",
        "\n",
        "    def _calculate_prototypes(self, support_embeddings, support_labels, n_way):\n",
        "         \"\"\" Calculates class prototypes from support embeddings \"\"\"\n",
        "         prototypes = torch.zeros(n_way, support_embeddings.size(1), device=support_embeddings.device)\n",
        "         for c in range(n_way):\n",
        "             class_embeddings = support_embeddings[support_labels == c]\n",
        "             if class_embeddings.size(0) > 0:\n",
        "                 prototypes[c] = class_embeddings.mean(dim=0)\n",
        "             # else: prototype remains zeros, distance will be high\n",
        "         return prototypes\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images, n_way):\n",
        "        n_support = support_images.size(0)\n",
        "        n_query = query_images.size(0)\n",
        "        all_images = torch.cat([support_images, query_images], dim=0)\n",
        "\n",
        "        # --- Get Features from Encoder ---\n",
        "        try:\n",
        "            # Ensure encoder is in eval mode if parts are frozen and BN stats shouldn't update\n",
        "            # However, if fine-tuning, it should be in train mode. Assume outer loop handles mode.\n",
        "            all_conv_features, all_global_embeddings = self.encoder(all_images)\n",
        "        except Exception as e:\n",
        "            print(f\"RuntimeError in encoder forward: {e}\")\n",
        "            # Return dummy outputs matching expected structure\n",
        "            dummy_logits = torch.zeros(n_query, n_way, device=all_images.device)\n",
        "            dummy_loss_info = {'clst_loss': torch.tensor(0.0, device=all_images.device),\n",
        "                               'diversity_loss': torch.tensor(0.0, device=all_images.device),\n",
        "                               'l1_loss': torch.tensor(0.0, device=all_images.device)}\n",
        "            dummy_explain_info = {'query_part_activations': None, 'class_part_profiles': {}}\n",
        "            return dummy_logits, dummy_loss_info, dummy_explain_info\n",
        "\n",
        "\n",
        "        support_conv_features = all_conv_features[:n_support]\n",
        "        query_conv_features = all_conv_features[n_support:]\n",
        "        support_global_embeddings = all_global_embeddings[:n_support]\n",
        "        query_global_embeddings = all_global_embeddings[n_support:]\n",
        "\n",
        "        # --- Level 1: Global Reasoning ---\n",
        "        global_prototypes = self._calculate_prototypes(support_global_embeddings, support_labels, n_way)\n",
        "        # Negative squared Euclidean distance as logits\n",
        "        dist_global = torch.cdist(query_global_embeddings, global_prototypes)**2\n",
        "        logits_global = -dist_global # Higher score = closer\n",
        "\n",
        "        # --- Level 2: Part Reasoning ---\n",
        "        logits_part_sim = torch.zeros(n_query, n_way, device=query_images.device)\n",
        "        query_part_activations_list = []\n",
        "        class_part_profiles = {} # Store {class_idx: profile_tensor}\n",
        "\n",
        "        # Calculate class profiles first (once per class)\n",
        "        for c in range(n_way):\n",
        "            class_support_indices = torch.where(support_labels == c)[0]\n",
        "            if len(class_support_indices) > 0:\n",
        "                 class_features = support_conv_features[class_support_indices]\n",
        "                 class_part_profiles[c] = self.part_prototype_layer.calculate_class_profile(class_features)\n",
        "            else:\n",
        "                 # Handle case where a class has no support images (shouldn't happen with good sampling)\n",
        "                 class_part_profiles[c] = torch.zeros(self.part_prototype_layer.num_parts, device=query_images.device)\n",
        "\n",
        "        # Calculate query activations and compare to class profiles\n",
        "        for i in range(n_query):\n",
        "            query_feat_single = query_conv_features[i].unsqueeze(0) # [1, D, H, W]\n",
        "            # Calculate activation profile for this single query image\n",
        "            act_q = self.part_prototype_layer.calculate_query_activation(query_feat_single) # [M]\n",
        "            query_part_activations_list.append(act_q)\n",
        "\n",
        "            # Calculate similarity score between query activation and each class profile\n",
        "            for c in range(n_way):\n",
        "                sim_score = self.part_prototype_layer.calculate_part_similarity_score(act_q, class_part_profiles[c])\n",
        "                logits_part_sim[i, c] = sim_score # Cosine similarity score\n",
        "\n",
        "        # --- Combine Level 1 and Level 2 ---\n",
        "        if self.combination_method == 'add':\n",
        "            # Combine negative distance and positive similarity\n",
        "            # Scaling might be needed depending on relative magnitudes\n",
        "            final_logits = logits_global + self.lambda_part_sim * logits_part_sim\n",
        "        elif self.combination_method == 'mlp' and self.combiner_mlp is not None:\n",
        "            # Reshape for MLP: Input features are [global_logit, part_sim_logit]\n",
        "            combined_input = torch.stack([logits_global, logits_part_sim], dim=-1) # [N_query, N_way, 2]\n",
        "            flat_input = combined_input.view(-1, 2) # [N_query * N_way, 2]\n",
        "            flat_output = self.combiner_mlp(flat_input) # [N_query * N_way, 1]\n",
        "            final_logits = flat_output.view(n_query, n_way) # [N_query, N_way]\n",
        "        else: # Default to global only if combination method is unknown or MLP missing\n",
        "             final_logits = logits_global\n",
        "\n",
        "        # --- Calculate Part Prototype Losses (for regularization during training) ---\n",
        "        # Use all features (support + query) for cluster loss calculation\n",
        "        reg_features = torch.cat([support_conv_features, query_conv_features], dim=0)\n",
        "        clst_loss = self.part_prototype_layer.calculate_cluster_loss(reg_features)\n",
        "        diversity_loss = self.part_prototype_layer.calculate_diversity_loss()\n",
        "        # L1 loss could be applied to query activations if needed\n",
        "        # l1_loss = self.part_prototype_layer.calculate_l1_loss(torch.stack(query_part_activations_list)) if query_part_activations_list else torch.tensor(0.0)\n",
        "        l1_loss = torch.tensor(0.0, device=final_logits.device) # Keep disabled for now\n",
        "\n",
        "        loss_info = {'clst_loss': clst_loss, 'diversity_loss': diversity_loss, 'l1_loss': l1_loss}\n",
        "        explanation_info = {\n",
        "            'query_part_activations': torch.stack(query_part_activations_list, dim=0) if query_part_activations_list else None, # [N_query, M]\n",
        "            'class_part_profiles': class_part_profiles # Dict {class_idx: tensor[M]}\n",
        "        }\n",
        "\n",
        "        return final_logits, loss_info, explanation_info\n",
        "\n",
        "    # --- Helper for Manual Grad-CAM ---\n",
        "    # This calculates the specific score needed for the backward pass in Grad-CAM\n",
        "    def get_part_activation_score(self, image_tensor, target_part_index):\n",
        "        \"\"\"\n",
        "        Calculates the activation score for a *single* target part prototype\n",
        "        given a *single* input image tensor. Required for manual Grad-CAM.\n",
        "        Ensures gradients can flow back from this score.\n",
        "        \"\"\"\n",
        "        if image_tensor.dim() != 4 or image_tensor.size(0) != 1:\n",
        "            raise ValueError(\"Input tensor must be [1, C, H, W]\")\n",
        "        if not (0 <= target_part_index < self.part_prototype_layer.num_parts):\n",
        "            raise ValueError(f\"Invalid target_part_index: {target_part_index}\")\n",
        "\n",
        "        # --- Re-compute relevant parts WITH gradient tracking ---\n",
        "        # 1. Get convolutional features\n",
        "        # NOTE: Ensure encoder is in appropriate mode (eval/train) outside this function\n",
        "        conv_features, _ = self.encoder(image_tensor) # [1, D, H, W]\n",
        "\n",
        "        # 2. Calculate patch similarities ONLY for the target prototype\n",
        "        B, D, H, W = conv_features.shape\n",
        "        N_PATCHES_Q = H * W\n",
        "        if N_PATCHES_Q <= 0: return torch.tensor(0.0, device=image_tensor.device, requires_grad=True) # Handle empty feature map\n",
        "\n",
        "        patches = conv_features.permute(0, 2, 3, 1).reshape(N_PATCHES_Q, D) # [HW, D]\n",
        "        target_prototype = self.part_prototype_layer.part_prototypes[target_part_index].unsqueeze(0) # [1, D]\n",
        "\n",
        "        # Similarity between all patches and the ONE target prototype\n",
        "        sim_matrix = self.part_prototype_layer._calculate_similarity(patches, target_prototype) # [HW, 1]\n",
        "\n",
        "        # 3. Find the maximum similarity (this is our target score)\n",
        "        max_similarity_score, _ = torch.max(sim_matrix, dim=0) # [1]\n",
        "\n",
        "        # Return the scalar score, ensuring it requires grad\n",
        "        return max_similarity_score.squeeze()"
      ],
      "metadata": {
        "id": "QLlFBGbQenvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Manual Grad-CAM Calculation ---\n",
        "# -------------------------------------\n",
        "\n",
        "# Store hook results globally (or pass via lists/objects)\n",
        "forward_features = {}\n",
        "backward_gradients = {}\n",
        "\n",
        "def setup_hooks(model, target_layer_name):\n",
        "    \"\"\" Finds the target layer and sets up forward/backward hooks. \"\"\"\n",
        "    target_layer = None\n",
        "    try:\n",
        "        # Navigate nested modules if necessary (e.g., 'encoder.layer4')\n",
        "        module_names = target_layer_name.split('.')\n",
        "        m = model\n",
        "        for name in module_names:\n",
        "            m = getattr(m, name)\n",
        "        target_layer = m\n",
        "    except AttributeError:\n",
        "        print(f\"ERROR: Could not find target layer '{target_layer_name}' in the model.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Clear previous hook data\n",
        "    forward_features.clear()\n",
        "    backward_gradients.clear()\n",
        "\n",
        "    def forward_hook(module, input, output):\n",
        "        # Store output activation map. Use layer name as key for safety.\n",
        "        # Detach might not be needed if backward hook handles requires_grad correctly.\n",
        "        forward_features[target_layer_name] = output # Store tensor directly\n",
        "\n",
        "    def backward_hook(module, grad_input, grad_output):\n",
        "        # Store the gradient flowing OUT of the layer (input to the next layer backward)\n",
        "        # grad_output is a tuple, usually take the first element\n",
        "        backward_gradients[target_layer_name] = grad_output[0] # Store tensor directly\n",
        "\n",
        "    # Register hooks\n",
        "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
        "    backward_handle = target_layer.register_full_backward_hook(backward_hook) # Use full backward hook\n",
        "\n",
        "    print(f\"Hooks registered on: {target_layer_name} ({type(target_layer)})\")\n",
        "    return target_layer, forward_handle, backward_handle\n",
        "\n",
        "def calculate_gradcam_manual(model, target_layer_name, input_tensor, target_part_index):\n",
        "    \"\"\"\n",
        "    Calculates Grad-CAM manually for a specific part activation.\n",
        "\n",
        "    Args:\n",
        "        model (HEPN): The trained HEPN model.\n",
        "        target_layer_name (str): Name of the target conv layer (e.g., 'encoder.layer4').\n",
        "        input_tensor (torch.Tensor): Input image tensor [1, C, H, W].\n",
        "        target_part_index (int): Index of the part prototype to visualize.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The Grad-CAM heatmap (H, W), or None if failed.\n",
        "    \"\"\"\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    target_layer, f_handle, b_handle = setup_hooks(model, target_layer_name)\n",
        "    if target_layer is None:\n",
        "        return None # Hook setup failed\n",
        "\n",
        "    # --- Perform forward and backward pass ---\n",
        "    try:\n",
        "        # Zero gradients before backward pass\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Calculate the specific score we want to visualize (ensures grad tracking)\n",
        "        # This requires requires_grad=True context\n",
        "        score = model.get_part_activation_score(input_tensor, target_part_index)\n",
        "\n",
        "        # Propagate gradients back from this score\n",
        "        score.backward()\n",
        "\n",
        "        # --- Retrieve activations and gradients from hooks ---\n",
        "        activations = forward_features.get(target_layer_name)\n",
        "        gradients = backward_gradients.get(target_layer_name)\n",
        "\n",
        "        if activations is None or gradients is None:\n",
        "            print(\"Error: Failed to capture activations or gradients from hooks.\")\n",
        "            return None\n",
        "\n",
        "        # Ensure tensors are on CPU for numpy conversion later if needed\n",
        "        activations = activations.detach().cpu() # [1, Channels, H_feat, W_feat]\n",
        "        gradients = gradients.detach().cpu()     # [1, Channels, H_feat, W_feat]\n",
        "\n",
        "        # --- Calculate Grad-CAM ---\n",
        "        # 1. Global Average Pooling of Gradients (alpha weights)\n",
        "        pooled_gradients = torch.mean(gradients, dim=[2, 3], keepdim=True) # [1, C, 1, 1]\n",
        "\n",
        "        # 2. Weighted Combination of Activations\n",
        "        # Multiply activations by weights channel-wise and sum across channels\n",
        "        weighted_activations = activations * pooled_gradients # Broadcasting: [1, C, Hf, Wf]\n",
        "        heatmap = weighted_activations.sum(dim=1, keepdim=True) # [1, 1, Hf, Wf]\n",
        "\n",
        "        # 3. Apply ReLU\n",
        "        heatmap = F.relu(heatmap)\n",
        "\n",
        "        # Remove batch and channel dimensions\n",
        "        heatmap = heatmap.squeeze() # [Hf, Wf]\n",
        "\n",
        "        # --- Post-processing ---\n",
        "        # Resize heatmap to original image size (optional, often done)\n",
        "        heatmap = heatmap.numpy() # Convert to numpy\n",
        "        # Ensure input_tensor is on CPU for shape access\n",
        "        input_h, input_w = input_tensor.shape[2:]\n",
        "        heatmap = cv2.resize(heatmap, (input_w, input_h))\n",
        "\n",
        "        # Normalize heatmap (0 to 1)\n",
        "        heatmap_min, heatmap_max = np.min(heatmap), np.max(heatmap)\n",
        "        if heatmap_max > heatmap_min:\n",
        "             heatmap = (heatmap - heatmap_min) / (heatmap_max - heatmap_min)\n",
        "        else:\n",
        "             heatmap = np.zeros_like(heatmap) # Avoid division by zero if heatmap is flat\n",
        "\n",
        "        return heatmap\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during manual Grad-CAM calculation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "    finally:\n",
        "        # --- CRITICAL: Remove hooks ---\n",
        "        if f_handle: f_handle.remove()\n",
        "        if b_handle: b_handle.remove()\n",
        "        # Clear stored data just in case\n",
        "        forward_features.clear()\n",
        "        backward_gradients.clear()\n",
        "        # print(\"Hooks removed.\")"
      ],
      "metadata": {
        "id": "YLYoBoFtyH64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Training & Evaluation (Adapted for HEPN) ---\n",
        "# -------------------------------------\n",
        "\n",
        "# --- Label Smoothing Loss (Unchanged) ---\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            # Ensure target is correct shape for scatter_\n",
        "            target_reshaped = target.data.unsqueeze(1)\n",
        "            true_dist.scatter_(1, target_reshaped, self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "# --- Setup Optimizer for HEPN ---\n",
        "def setup_optimizer_hepn(model, lr_backbone, lr_head_global, lr_part_prototypes, lr_combiner, wd):\n",
        "     \"\"\" Creates optimizer with differential learning rates for HEPN components \"\"\"\n",
        "     encoder_backbone_params, encoder_head_params = model.encoder.get_trainable_parameters()\n",
        "     # Get part prototype parameters explicitly\n",
        "     part_prototype_params = [model.part_prototype_layer.part_prototypes]\n",
        "\n",
        "     # Add combiner params if MLP method is used and it exists\n",
        "     combiner_params = []\n",
        "     if hasattr(model, 'combiner_mlp') and model.combiner_mlp is not None:\n",
        "         combiner_params = list(model.combiner_mlp.parameters())\n",
        "         print(f\"Adding {len(combiner_params)} combiner MLP parameters to optimizer.\")\n",
        "\n",
        "     param_groups = [\n",
        "          {'params': encoder_backbone_params, 'lr': lr_backbone, 'weight_decay': wd, 'name': 'backbone'},\n",
        "          {'params': encoder_head_params, 'lr': lr_head_global, 'weight_decay': wd, 'name': 'global_head'},\n",
        "          {'params': part_prototype_params, 'lr': lr_part_prototypes, 'weight_decay': 0.0, 'name': 'part_protos'}, # Often no WD on prototypes\n",
        "     ]\n",
        "     if combiner_params:\n",
        "          param_groups.append({'params': combiner_params, 'lr': lr_combiner, 'weight_decay': wd, 'name': 'combiner'})\n",
        "\n",
        "     # Filter out empty parameter groups (e.g., if head is frozen)\n",
        "     param_groups = [pg for pg in param_groups if pg['params']]\n",
        "\n",
        "     print(f\"Optimizer Groups:\")\n",
        "     total_params = 0\n",
        "     for pg in param_groups:\n",
        "         pg_params = sum(p.numel() for p in pg['params'])\n",
        "         total_params += pg_params\n",
        "         print(f\"  - {pg['name']}: {len(pg['params'])} tensors, {pg_params:,} params, LR {pg['lr']:.1e}, WD {pg.get('weight_decay', 0.0):.1e}\")\n",
        "     print(f\"Total trainable parameters in optimizer: {total_params:,}\")\n",
        "\n",
        "     optimizer = optim.AdamW(param_groups) # WD applied per group by AdamW\n",
        "     return optimizer\n",
        "\n",
        "\n",
        "# --- Modified Training Step ---\n",
        "def train_step_hepn(model, optimizer, cls_loss_fn, support_images, support_labels, query_images, query_labels, n_way, gradient_clip_norm, loss_weights):\n",
        "    model.train() # Ensure model is in training mode\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    support_images = support_images.to(DEVICE, non_blocking=True)\n",
        "    support_labels = support_labels.to(DEVICE, non_blocking=True)\n",
        "    query_images = query_images.to(DEVICE, non_blocking=True)\n",
        "    query_labels = query_labels.to(DEVICE, non_blocking=True)\n",
        "\n",
        "    # --- Forward Pass ---\n",
        "    final_logits, loss_info, _ = model(support_images, support_labels, query_images, n_way)\n",
        "\n",
        "    if final_logits is None or torch.isnan(final_logits).any() or torch.isinf(final_logits).any():\n",
        "         print(\"Warning: NaN/Inf final logits detected during training. Skipping batch.\")\n",
        "         nan_loss_info = {k: float('nan') for k in loss_info}\n",
        "         return {'loss': float('nan'), 'acc': 0.0, 'cls_loss': float('nan'), **nan_loss_info}\n",
        "\n",
        "    # --- Calculate Losses ---\n",
        "    cls_loss = cls_loss_fn(final_logits, query_labels)\n",
        "\n",
        "    # Combine classification loss with regularization losses using weights\n",
        "    clst_term = loss_weights.get('clst', 0.0) * loss_info.get('clst_loss', torch.tensor(0.0, device=DEVICE))\n",
        "    div_term = loss_weights.get('diversity', 0.0) * loss_info.get('diversity_loss', torch.tensor(0.0, device=DEVICE))\n",
        "    l1_term = loss_weights.get('l1', 0.0) * loss_info.get('l1_loss', torch.tensor(0.0, device=DEVICE))\n",
        "\n",
        "    total_loss = cls_loss + clst_term + div_term + l1_term\n",
        "\n",
        "    # --- Backward and Optimize ---\n",
        "    if torch.isnan(total_loss):\n",
        "        print(\"Warning: NaN total loss calculated. Skipping backward pass.\")\n",
        "        debug_loss_info = {k: v.item() if torch.is_tensor(v) and v.numel() == 1 else float('nan') for k, v in loss_info.items()}\n",
        "        return {'loss': float('nan'), 'acc': 0.0, 'cls_loss': cls_loss.item() if torch.is_tensor(cls_loss) else float('nan'), **debug_loss_info}\n",
        "\n",
        "    total_loss.backward()\n",
        "\n",
        "    # Gradient Clipping (applied to all parameters)\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Calculate Accuracy ---\n",
        "    with torch.no_grad():\n",
        "        predictions = torch.argmax(final_logits, dim=1)\n",
        "        accuracy = torch.mean((predictions == query_labels).float())\n",
        "\n",
        "    # Return detailed loss info (scalar values)\n",
        "    loss_dict_items = {k: v.item() if torch.is_tensor(v) and v.numel() == 1 else 0.0 for k, v in loss_info.items()}\n",
        "    loss_dict = {\n",
        "        'loss': total_loss.item(),\n",
        "        'acc': accuracy.item(),\n",
        "        'cls_loss': cls_loss.item(),\n",
        "        **loss_dict_items\n",
        "    }\n",
        "    return loss_dict\n",
        "\n",
        "\n",
        "# --- Modified Evaluation Step ---\n",
        "def evaluate_step_hepn(model, support_images, support_labels, query_images, query_labels, n_way):\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "\n",
        "    support_images = support_images.to(DEVICE)\n",
        "    support_labels = support_labels.to(DEVICE)\n",
        "    query_images = query_images.to(DEVICE)\n",
        "    query_labels = query_labels.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Forward pass - don't need loss info or explanation details for accuracy\n",
        "        final_logits, _, _ = model(support_images, support_labels, query_images, n_way)\n",
        "\n",
        "        if final_logits is None or torch.isnan(final_logits).any() or torch.isinf(final_logits).any():\n",
        "            print(\"Warning: NaN/Inf detected in final_logits during evaluation.\")\n",
        "            return 0.0 # Return 0 accuracy or handle appropriately\n",
        "\n",
        "    predictions = torch.argmax(final_logits, dim=1)\n",
        "    accuracy = torch.mean((predictions == query_labels).float())\n",
        "    return accuracy.item()\n",
        "\n",
        "\n",
        "# --- Modified Training Loop ---\n",
        "def main_training_loop_hepn(model, train_sampler, test_sampler, meta_train_full_dataset,\n",
        "                           n_train_episodes, n_test_episodes,\n",
        "                           lr_backbone, lr_head_global, lr_part_prototypes, lr_combiner, wd,\n",
        "                           label_smoothing, grad_clip_norm,\n",
        "                           lambda_clst, lambda_diversity, lambda_l1,\n",
        "                           part_proj_interval):\n",
        "\n",
        "    optimizer = setup_optimizer_hepn(model, lr_backbone, lr_head_global, lr_part_prototypes, lr_combiner, wd)\n",
        "    cls_loss_fn = LabelSmoothingLoss(classes=train_sampler.n_way, smoothing=label_smoothing).to(DEVICE)\n",
        "\n",
        "    loss_weights = {'clst': lambda_clst, 'diversity': lambda_diversity, 'l1': lambda_l1}\n",
        "    print(f\"Using Loss Weights: {loss_weights}\")\n",
        "\n",
        "    # Dynamic history keys based on used losses\n",
        "    train_history = {'loss': [], 'acc': [], 'cls_loss': []}\n",
        "    for key, weight in loss_weights.items():\n",
        "        if weight > 0: train_history[f'{key}_loss'] = []\n",
        "\n",
        "    test_accuracies = []\n",
        "    best_test_acc = 0.0\n",
        "    test_eval_interval = 500\n",
        "    log_interval = 100\n",
        "    best_model_path = os.path.join(BASE_PATH, \"best_hepn_model_manual_cam.pth\") # Updated name\n",
        "\n",
        "    print(\"\\n--- Starting HEPN Meta-Training (Manual CAM Version) ---\")\n",
        "    print(f\"Config: N={N_WAY}, K={K_SHOT}, Q={N_QUERY}, Train Eps={n_train_episodes}, Test Eps={n_test_episodes}\")\n",
        "    print(f\"LRs: BB={lr_backbone:.1e}, GlobalH={lr_head_global:.1e}, Parts={lr_part_prototypes:.1e}, Comb={lr_combiner:.1e}\")\n",
        "    print(f\"Loss Lambdas -> Clst:{loss_weights.get('clst',0):.2f}, Div:{loss_weights.get('diversity',0):.2f}, L1:{loss_weights.get('l1',0):.2f}, PartSimCombine:{model.lambda_part_sim:.2f}\")\n",
        "    print(f\"Projection Interval: {part_proj_interval} episodes\")\n",
        "\n",
        "    pbar = tqdm(range(n_train_episodes))\n",
        "    for episode_idx in pbar:\n",
        "        try:\n",
        "            support_images, support_labels, query_images, query_labels = train_sampler.sample()\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping train episode {episode_idx+1} due to sampler error: {e}\")\n",
        "            continue # Skip episode if sampling fails\n",
        "\n",
        "        # --- Training Step ---\n",
        "        loss_dict = train_step_hepn(model, optimizer, cls_loss_fn, support_images, support_labels,\n",
        "                                   query_images, query_labels, train_sampler.n_way, grad_clip_norm, loss_weights)\n",
        "\n",
        "        # --- Log Training Stats ---\n",
        "        if not math.isnan(loss_dict['loss']):\n",
        "             for key in train_history.keys():\n",
        "                 if key in loss_dict and not math.isnan(loss_dict[key]):\n",
        "                     train_history[key].append(loss_dict[key])\n",
        "\n",
        "             if (episode_idx + 1) % log_interval == 0 and train_history['loss']:\n",
        "                 desc = f\"Ep {episode_idx+1}\"\n",
        "                 for key, values in train_history.items():\n",
        "                      if values:\n",
        "                           avg_val = np.mean(values[-log_interval:])\n",
        "                           key_short = \"\".join(w[0].upper() for w in key.split('_'))\n",
        "                           if key == 'acc': key_short = 'Acc'\n",
        "                           if key == 'loss': key_short = 'L'\n",
        "                           desc += f\" | {key_short}:{avg_val:.3f}\"\n",
        "                 desc += f\" | BestT:{best_test_acc:.4f}\"\n",
        "                 pbar.set_description(desc)\n",
        "\n",
        "        # --- Evaluate and Save Best Model ---\n",
        "        if (episode_idx + 1) % test_eval_interval == 0 or episode_idx == n_train_episodes - 1:\n",
        "             eval_eps = n_test_episodes // 2 if episode_idx != n_train_episodes - 1 else n_test_episodes\n",
        "             current_test_acc = evaluate_on_test_set_hepn(model, test_sampler, eval_eps)\n",
        "             if not math.isnan(current_test_acc):\n",
        "                 test_accuracies.append(current_test_acc)\n",
        "                 print(f\" | Test Acc @ Ep {episode_idx+1}: {current_test_acc:.4f}\")\n",
        "                 if current_test_acc > best_test_acc:\n",
        "                      best_test_acc = current_test_acc\n",
        "                      print(f\"*** New best test accuracy: {best_test_acc:.4f}. Saving model to {best_model_path} ***\")\n",
        "                      try:\n",
        "                          torch.save(model.state_dict(), best_model_path)\n",
        "                      except Exception as e:\n",
        "                          print(f\"Error saving model: {e}\")\n",
        "             else:\n",
        "                  print(f\" | Test Acc @ Ep {episode_idx+1}: NaN\")\n",
        "             model.train() # Ensure model is back in train mode\n",
        "\n",
        "\n",
        "        # --- Part Prototype Projection ---\n",
        "        if (episode_idx > 0 and (episode_idx + 1) % part_proj_interval == 0):\n",
        "             if meta_train_full_dataset is not None:\n",
        "                  print(f\"\\n--- Projecting prototypes at episode {episode_idx + 1} ---\")\n",
        "                  model.part_prototype_layer.project_part_prototypes(\n",
        "                       dataset_for_proj=meta_train_full_dataset,\n",
        "                       encoder_model=model.encoder,\n",
        "                       device=DEVICE,\n",
        "                       batch_size=PROJECTION_BATCH_SIZE\n",
        "                  )\n",
        "                  model.train() # Ensure model back in train mode\n",
        "             else:\n",
        "                  print(f\"Warning: Cannot project prototypes at episode {episode_idx + 1}, meta_train_full_dataset is None.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Finished HEPN Meta-Training ---\")\n",
        "    print(f\"Best meta-test accuracy achieved: {best_test_acc:.4f}\")\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    print(\"\\nRunning final evaluation on full test set...\")\n",
        "    final_test_acc = evaluate_on_test_set_hepn(model, test_sampler, n_test_episodes)\n",
        "    print(f\"Final Meta-Test Accuracy: {final_test_acc:.4f}\")\n",
        "\n",
        "    # --- Plotting ---\n",
        "    num_plots = len(train_history)\n",
        "    if num_plots > 0:\n",
        "        plt.figure(figsize=(min(18, 5 * num_plots) , 5))\n",
        "        plot_idx = 1\n",
        "        for key, values in train_history.items():\n",
        "             if values: # Only plot if data exists\n",
        "                 plt.subplot(1, num_plots, plot_idx)\n",
        "                 plt.plot(values, alpha=0.6, label='Raw')\n",
        "                 if len(values) >= log_interval:\n",
        "                     rolling_avg = pd.Series(values).rolling(log_interval, min_periods=log_interval//2).mean()\n",
        "                     plt.plot(rolling_avg, label=f'Avg {log_interval}eps', linewidth=2)\n",
        "                 plt.title(f'Training {key.replace(\"_\", \" \").title()}')\n",
        "                 plt.xlabel('Episode')\n",
        "                 plt.ylabel(key)\n",
        "                 if 'acc' in key: plt.ylim(0, max(1.0, np.max(values)*1.1) if values else 1.0)\n",
        "                 plt.grid(True, linestyle='--', alpha=0.6)\n",
        "                 plt.legend()\n",
        "                 plot_idx += 1\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return model # Return the trained model\n",
        "\n",
        "\n",
        "# --- Evaluation Loop Helper ---\n",
        "def evaluate_on_test_set_hepn(model, test_sampler, n_episodes):\n",
        "    all_accuracies = []\n",
        "    pbar = tqdm(range(n_episodes), desc=\"Meta-Testing\", leave=False)\n",
        "    for i in pbar:\n",
        "        try:\n",
        "             support_images, support_labels, query_images, query_labels = test_sampler.sample()\n",
        "        except ValueError as e:\n",
        "             print(f\"Skipping test episode due to sampler error: {e}\")\n",
        "             continue\n",
        "\n",
        "        acc = evaluate_step_hepn(model, support_images, support_labels, query_images, query_labels, test_sampler.n_way)\n",
        "        if not math.isnan(acc):\n",
        "            all_accuracies.append(acc)\n",
        "            pbar.set_postfix({\"Avg Acc\": f\"{np.mean(all_accuracies):.4f}\"})\n",
        "\n",
        "    if not all_accuracies: return float('nan') # Return NaN if no episodes were successful\n",
        "    return np.mean(all_accuracies)"
      ],
      "metadata": {
        "id": "jSfDD0NXyO0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Visualization (with Manual Grad-CAM) ---\n",
        "# -------------------------------------\n",
        "\n",
        "# Utility function to overlay CAM - similar to pytorch-gradcam's show_cam_on_image\n",
        "def show_cam_on_image(img: np.ndarray,\n",
        "                      mask: np.ndarray,\n",
        "                      use_rgb: bool = False,\n",
        "                      colormap: int = cv2.COLORMAP_JET,\n",
        "                      image_weight: float = 0.5) -> np.ndarray:\n",
        "    \"\"\" Overlays a CAM mask onto an image.\n",
        "        Args:\n",
        "            img: Input image Numpy array (H, W, C). Float 0-1.\n",
        "            mask: CAM mask Numpy array (H, W). Float 0-1.\n",
        "            use_rgb: Whether to use an RGB or BGR heatmap map.\n",
        "            colormap: OpenCV colormap to use.\n",
        "            image_weight: Weight for blending the original image.\n",
        "        Returns:\n",
        "            Numpy array: Image with CAM overlay.\n",
        "    \"\"\"\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
        "    if use_rgb:\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "\n",
        "    if np.max(img) > 1:\n",
        "        raise Exception(\"The input image should np.float32 in the range [0, 1]\")\n",
        "    if image_weight < 0 or image_weight > 1:\n",
        "        raise Exception(f\"image_weight should be in the range [0, 1]. Got: {image_weight}\")\n",
        "\n",
        "    cam = (1 - image_weight) * heatmap + image_weight * img\n",
        "    cam = cam / np.max(cam) # Normalize to 0-1 range\n",
        "    return np.uint8(255 * cam)\n",
        "\n",
        "\n",
        "# Inverse transform for visualization\n",
        "inv_normalize_imagenet = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")\n",
        "\n",
        "def preprocess_image_for_viz(tensor_img):\n",
        "    \"\"\" Prepares tensor for showing with CAM overlay (float 0-1, HWC) \"\"\"\n",
        "    if tensor_img is None: return None\n",
        "    img = tensor_img.detach().cpu()\n",
        "    img = inv_normalize_imagenet(img)\n",
        "    img = img.permute(1, 2, 0) # HWC\n",
        "    img = img.clamp(0, 1)\n",
        "    return img.numpy()\n",
        "\n",
        "\n",
        "def visualize_explanation_hepn_manual_gradcam(model, test_sampler, target_layer_name, num_explanations=1, top_k_parts=3):\n",
        "    \"\"\"\n",
        "    Visualizes HEPN explanations using MANUALLY calculated Grad-CAM.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Visualizing HEPN Explanations (Manual Grad-CAM) for {num_explanations} Test Episodes ---\")\n",
        "    model.eval() # Ensure evaluation mode\n",
        "\n",
        "    for vis_idx in range(num_explanations):\n",
        "        print(f\"\\n--- Explanation Example {vis_idx + 1}/{num_explanations} ---\")\n",
        "        try:\n",
        "            support_imgs, support_lbls, query_imgs, query_lbls = test_sampler.sample()\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping visualization {vis_idx+1} due to sampler error: {e}\")\n",
        "            continue\n",
        "        if query_imgs is None or query_imgs.size(0) == 0: continue\n",
        "\n",
        "        query_idx_to_explain = random.randrange(query_imgs.size(0))\n",
        "        support_images_dev = support_imgs.to(DEVICE)\n",
        "        support_labels_dev = support_lbls.to(DEVICE)\n",
        "        query_image_single_tensor = query_imgs[query_idx_to_explain]\n",
        "        query_image_single_input = query_image_single_tensor.unsqueeze(0).to(DEVICE) # [1, C, H, W]\n",
        "        true_label = query_lbls[query_idx_to_explain].item()\n",
        "        n_way = test_sampler.n_way\n",
        "\n",
        "        # --- Get Model Outputs & Explanation Info ---\n",
        "        predicted_label = -1\n",
        "        query_part_activation = None\n",
        "        class_part_profiles = {}\n",
        "        with torch.no_grad(): # Get predictions without grad\n",
        "             try:\n",
        "                 final_logits, _, explanation_info = model(\n",
        "                     support_images_dev, support_labels_dev, query_image_single_input, n_way\n",
        "                 )\n",
        "                 if final_logits is not None and final_logits.numel() > 0:\n",
        "                      predicted_label = torch.argmax(final_logits, dim=1).item()\n",
        "                      query_part_activation = explanation_info.get('query_part_activations') # [1, M]\n",
        "                      if query_part_activation is not None: query_part_activation = query_part_activation.squeeze(0).cpu() # [M]\n",
        "                      class_part_profiles = explanation_info.get('class_part_profiles', {})\n",
        "                 else: print(\"Warning: Model forward pass returned invalid logits.\")\n",
        "             except Exception as e:\n",
        "                  print(f\"Error during model forward pass for visualization: {e}\")\n",
        "                  import traceback; traceback.print_exc(); continue\n",
        "\n",
        "        print(f\"Explaining Query Img Idx: {query_idx_to_explain} | Pred: {predicted_label} | True: {true_label}\")\n",
        "\n",
        "        # --- Create Figure ---\n",
        "        num_bar_charts = 0\n",
        "        if query_part_activation is not None: num_bar_charts += 1\n",
        "        if class_part_profiles.get(predicted_label) is not None: num_bar_charts += 1\n",
        "        can_do_cam = query_part_activation is not None and query_part_activation.numel() > 0\n",
        "        actual_top_k_parts = min(top_k_parts, query_part_activation.numel()) if can_do_cam else 0\n",
        "        num_cam_plots = actual_top_k_parts\n",
        "        total_plots = 1 + num_bar_charts + num_cam_plots\n",
        "\n",
        "        if total_plots <= 1: N_COLS_VIS, N_ROWS_VIS = 1, 1\n",
        "        else: N_COLS_VIS = min(total_plots, 4); N_ROWS_VIS = math.ceil(total_plots / N_COLS_VIS)\n",
        "\n",
        "        plt.figure(figsize=(max(8, 4 * N_COLS_VIS), 3.5 * N_ROWS_VIS))\n",
        "        plot_idx = 1\n",
        "        suptitle = f\"HEPN Expl {vis_idx+1}: Q Img {query_idx_to_explain} (Pred: {predicted_label}, True: {true_label})\"\n",
        "        if can_do_cam: suptitle += f\" | Top {actual_top_k_parts} CAMs (Manual)\"\n",
        "        plt.suptitle(suptitle, fontsize=12, y=0.99)\n",
        "\n",
        "        # == Plot Query Image ==\n",
        "        ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "        query_img_np_viz = preprocess_image_for_viz(query_image_single_tensor)\n",
        "        if query_img_np_viz is not None: ax.imshow(query_img_np_viz)\n",
        "        ax.set_title(f\"Query Img {query_idx_to_explain}\\nTrue: {true_label} / Pred: {predicted_label}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # == Plot Bar Charts ==\n",
        "        top_activated_part_indices = []\n",
        "        if query_part_activation is not None and query_part_activation.numel() > 0:\n",
        "            ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "            sorted_vals, sorted_indices = torch.sort(query_part_activation, descending=True)\n",
        "            k_bar = min(10, query_part_activation.numel())\n",
        "            if k_bar > 0:\n",
        "                bar_vals, bar_indices = sorted_vals[:k_bar], sorted_indices[:k_bar]\n",
        "                top_activated_part_indices = sorted_indices[:actual_top_k_parts].tolist() # Store indices for CAM\n",
        "                ax.barh(range(k_bar), bar_vals.flip(dims=[0]), tick_label=[f\"P {i}\" for i in bar_indices.flip(dims=[0]).tolist()])\n",
        "                ax.set_title(f\"Query Top {k_bar} Part Acts\"); ax.tick_params(axis='y', labelsize=8)\n",
        "            else: ax.axis('off'); ax.text(0.5, 0.5, \"No activations\", ha='center')\n",
        "        elif num_bar_charts > 0: ax=plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1; ax.axis('off'); ax.text(0.5, 0.5, \"No activation data\", ha='center')\n",
        "\n",
        "        pred_class_profile = class_part_profiles.get(predicted_label)\n",
        "        if pred_class_profile is not None and pred_class_profile.numel() > 0:\n",
        "            ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "            pred_class_profile_cpu = pred_class_profile.cpu()\n",
        "            sorted_prof_vals, sorted_prof_indices = torch.sort(pred_class_profile_cpu, descending=True)\n",
        "            k_prof_bar = min(10, pred_class_profile_cpu.numel())\n",
        "            if k_prof_bar > 0:\n",
        "                 bar_prof_vals, bar_prof_indices = sorted_prof_vals[:k_prof_bar], sorted_prof_indices[:k_prof_bar]\n",
        "                 ax.barh(range(k_prof_bar), bar_prof_vals.flip(dims=[0]), tick_label=[f\"P {i}\" for i in bar_prof_indices.flip(dims=[0]).tolist()])\n",
        "                 ax.set_title(f\"Pred Cls {predicted_label} Top {k_prof_bar} Profile\"); ax.tick_params(axis='y', labelsize=8)\n",
        "            else: ax.axis('off'); ax.text(0.5, 0.5, \"No profile values\", ha='center')\n",
        "        elif num_bar_charts > 1: ax=plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1; ax.axis('off'); ax.text(0.5, 0.5, \"No profile data\", ha='center')\n",
        "\n",
        "\n",
        "        # == Generate and Plot Manual Grad-CAM ==\n",
        "        if can_do_cam and top_activated_part_indices:\n",
        "            print(f\"  Generating Manual Grad-CAM for top {len(top_activated_part_indices)} activated parts: {top_activated_part_indices}\")\n",
        "            input_image_np = query_img_np_viz # Use the HWC, 0-1 numpy image\n",
        "            if input_image_np is None: print(\"  Skipping CAMs as base image processing failed.\")\n",
        "            else:\n",
        "                for rank, part_idx in enumerate(top_activated_part_indices):\n",
        "                    if plot_idx > N_ROWS_VIS * N_COLS_VIS: print(f\"  Warning: Not enough plot slots...\"); break\n",
        "                    ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "\n",
        "                    # Calculate Grad-CAM manually\n",
        "                    # Pass the model, target layer name string, input tensor [1,C,H,W], and part index\n",
        "                    grayscale_cam = calculate_gradcam_manual(model,\n",
        "                                                             target_layer_name,\n",
        "                                                             query_image_single_input, # Use the [1,C,H,W] tensor\n",
        "                                                             part_idx)\n",
        "\n",
        "                    if grayscale_cam is not None:\n",
        "                        visualization = show_cam_on_image(input_image_np, grayscale_cam, use_rgb=True)\n",
        "                        ax.imshow(visualization)\n",
        "                        ax.set_title(f\"Part {part_idx} CAM (Rank {rank+1})\"); ax.axis('off')\n",
        "                    else:\n",
        "                         print(f\"  Failed to generate GradCAM for Part {part_idx}.\")\n",
        "                         ax.set_title(f\"GradCAM Fail P{part_idx}\"); ax.axis('off'); continue\n",
        "\n",
        "        # --- Cleanup and Show ---\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
        "        plt.show()\n",
        "\n",
        "    # End of visualization loop"
      ],
      "metadata": {
        "id": "l_Vaxp9iyPki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Main Execution ---\n",
        "# -------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility (optional)\n",
        "    seed = 42\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Might impact performance, but increases reproducibility\n",
        "        # torch.backends.cudnn.deterministic = True\n",
        "        # torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    print(\"1. Preparing CUB Data Splits...\")\n",
        "    # Ensure split file path uses BASE_PATH\n",
        "    meta_train_data, meta_test_data, meta_train_full_dataset_for_proj = prepare_cub_data_splits(\n",
        "        data_dir=DATA_DIR,\n",
        "        split_save_path=SPLIT_FILE,\n",
        "        force_resplit=FORCE_RESPLIT\n",
        "    )\n",
        "\n",
        "    if meta_train_data and meta_test_data and meta_train_full_dataset_for_proj:\n",
        "        print(f\"\\nUsing N_WAY={N_WAY}, K_SHOT={K_SHOT}, N_QUERY={N_QUERY} after potential adjustment.\")\n",
        "\n",
        "        print(\"\\n2. Creating Episode Samplers...\")\n",
        "        try:\n",
        "             train_sampler = EpisodeSampler(meta_train_data, N_WAY, K_SHOT, N_QUERY)\n",
        "             test_sampler = EpisodeSampler(meta_test_data, N_WAY, K_SHOT, N_QUERY)\n",
        "             print(f\"Train Sampler: {train_sampler.num_classes} classes.\")\n",
        "             print(f\"Test Sampler: {test_sampler.num_classes} classes.\")\n",
        "        except ValueError as e:\n",
        "             print(f\"\\n*** ERROR initializing samplers: {e} ***\")\n",
        "             print(\"Check N_WAY/K_SHOT/N_QUERY vs available classes/samples.\")\n",
        "             exit()\n",
        "\n",
        "        print(\"\\n3. Initializing HEPN Model...\")\n",
        "        encoder = EncoderHEPN(\n",
        "            embedding_dim_global=EMBEDDING_DIM_GLOBAL,\n",
        "            patch_feature_dim=EMBEDDING_DIM_PATCH, # Informational, uses ResNet's dim\n",
        "            pretrained=PRETRAINED, freeze_until=FREEZE_UNTIL_LAYER, dropout_rate=DROPOUT_RATE\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        actual_patch_dim = encoder.patch_feature_dim\n",
        "        print(f\"Instantiated Encoder with actual patch feature dim: {actual_patch_dim}\")\n",
        "\n",
        "        model = HEPN(encoder=encoder,\n",
        "                     num_parts=NUM_PART_PROTOTYPES,\n",
        "                     lambda_part_sim=LAMBDA_PART_SIM,\n",
        "                     combination_method='add' # or 'mlp'\n",
        "                    ).to(DEVICE)\n",
        "        print(f\"HEPN Model Initialized on {DEVICE}.\")\n",
        "        try:\n",
        "             trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "             total_params = sum(p.numel() for p in model.parameters())\n",
        "             print(f\"HEPN Trainable Params: {trainable_params:,} / Total Params: {total_params:,}\")\n",
        "        except Exception as e:\n",
        "             print(f\"Could not calculate parameter count: {e}\")\n",
        "\n",
        "\n",
        "        print(\"\\n4. Starting/Loading HEPN Training...\")\n",
        "        LOAD_SAVED_MODEL = False # Set True to attempt loading\n",
        "        model_path = os.path.join(BASE_PATH, \"best_hepn_model_manual_cam.pth\") # Ensure path consistency\n",
        "        trained_model = model\n",
        "\n",
        "        if LOAD_SAVED_MODEL and os.path.exists(model_path):\n",
        "            print(f\"Attempting to load saved model state from: {model_path}\")\n",
        "            try:\n",
        "                 # Use strict=True for reliable loading unless you know the architecture differs slightly\n",
        "                 model.load_state_dict(torch.load(model_path, map_location=DEVICE), strict=True)\n",
        "                 print(\"Successfully loaded saved model state.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"----------------------------------------------------\")\n",
        "                 print(f\"ERROR loading saved model: {e}\")\n",
        "                 print(f\"Model state dict NOT loaded. Training from scratch.\")\n",
        "                 print(f\"----------------------------------------------------\")\n",
        "                 LOAD_SAVED_MODEL = False # Force training if load fails\n",
        "        elif LOAD_SAVED_MODEL:\n",
        "            print(f\"Saved model path specified ({model_path}) but file not found. Training from scratch.\")\n",
        "            LOAD_SAVED_MODEL = False\n",
        "        else:\n",
        "            print(\"LOAD_SAVED_MODEL is False. Training from scratch.\")\n",
        "\n",
        "\n",
        "        # --- Training ---\n",
        "        if not LOAD_SAVED_MODEL:\n",
        "            print(\"Starting new training run...\")\n",
        "            trained_model = main_training_loop_hepn(\n",
        "                model=model,\n",
        "                train_sampler=train_sampler,\n",
        "                test_sampler=test_sampler,\n",
        "                meta_train_full_dataset=meta_train_full_dataset_for_proj,\n",
        "                n_train_episodes=N_TRAIN_EPISODES,\n",
        "                n_test_episodes=N_TEST_EPISODES,\n",
        "                lr_backbone=LR_BACKBONE,\n",
        "                lr_head_global=LR_HEAD_GLOBAL,\n",
        "                lr_part_prototypes=LR_PART_PROTOTYPES,\n",
        "                lr_combiner=LR_COMBINER,\n",
        "                wd=WEIGHT_DECAY,\n",
        "                label_smoothing=LABEL_SMOOTHING,\n",
        "                grad_clip_norm=GRADIENT_CLIP_NORM,\n",
        "                lambda_clst=LAMBDA_CLST,\n",
        "                lambda_diversity=LAMBDA_DIVERSITY,\n",
        "                lambda_l1=LAMBDA_L1,\n",
        "                part_proj_interval=PART_PROJECTION_INTERVAL\n",
        "            )\n",
        "            print(\"Training finished.\")\n",
        "        else:\n",
        "             print(\"Skipping training, using loaded model.\")\n",
        "\n",
        "\n",
        "        print(\"\\n5. Visualizing HEPN Explanation with Manual Grad-CAM...\")\n",
        "        if trained_model is not None:\n",
        "             visualize_explanation_hepn_manual_gradcam(\n",
        "                  model=trained_model,\n",
        "                  test_sampler=test_sampler,\n",
        "                  target_layer_name=TARGET_LAYER_NAME, # Pass the target layer name string\n",
        "                  num_explanations=5,\n",
        "                  top_k_parts=4\n",
        "             )\n",
        "        else:\n",
        "             print(\"ERROR: No valid model available for visualization.\")\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- ERROR: Failed to prepare data. Check paths, data integrity, and split parameters. Exiting. ---\")\n",
        "\n",
        "    print(\"\\n--- Script Execution Finished ---\")"
      ],
      "metadata": {
        "id": "i6tG1SfDyUwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_explanation_hepn_manual_gradcam(\n",
        "                  model=trained_model,\n",
        "                  test_sampler=test_sampler,\n",
        "                  target_layer_name=TARGET_LAYER_NAME, # Pass the target layer name string\n",
        "                  num_explanations=5,\n",
        "                  top_k_parts=4)"
      ],
      "metadata": {
        "id": "uLCkWSqJL4be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUqnbmMSOcVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import cv2 # Make sure cv2 is imported if not already\n",
        "\n",
        "# Assume all other necessary imports and definitions (like preprocess_image_for_viz,\n",
        "# calculate_gradcam_manual, show_cam_on_image, DEVICE, etc.) are present above.\n",
        "\n",
        "def visualize_explanation_hepn_manual_gradcam(\n",
        "    model,\n",
        "    test_sampler,\n",
        "    target_layer_name,\n",
        "    num_explanations=1,\n",
        "    top_k_parts=3,\n",
        "    num_support_cams_per_class=2 # <-- New parameter\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Visualizes HEPN explanations using MANUALLY calculated Grad-CAM for query\n",
        "    AND selected support images of the predicted class.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Visualizing HEPN Explanations (Manual Grad-CAM) for {num_explanations} Test Episodes ---\")\n",
        "    print(f\"    (Including CAM for up to {num_support_cams_per_class} support images per explanation)\")\n",
        "    model.eval() # Ensure evaluation mode\n",
        "\n",
        "    for vis_idx in range(num_explanations):\n",
        "        print(f\"\\n--- Explanation Example {vis_idx + 1}/{num_explanations} ---\")\n",
        "        try:\n",
        "            support_imgs, support_lbls, query_imgs, query_lbls = test_sampler.sample()\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping visualization {vis_idx+1} due to sampler error: {e}\")\n",
        "            continue\n",
        "        if query_imgs is None or query_imgs.size(0) == 0: continue\n",
        "\n",
        "        query_idx_to_explain = random.randrange(query_imgs.size(0))\n",
        "        support_images_dev = support_imgs.to(DEVICE)\n",
        "        support_labels_dev = support_lbls.to(DEVICE)\n",
        "        # Keep original support tensors on CPU for potential visualization later\n",
        "        support_images_cpu = support_imgs.cpu()\n",
        "        support_labels_cpu = support_lbls.cpu()\n",
        "\n",
        "        query_image_single_tensor = query_imgs[query_idx_to_explain]\n",
        "        query_image_single_input = query_image_single_tensor.unsqueeze(0).to(DEVICE) # [1, C, H, W]\n",
        "        true_label = query_lbls[query_idx_to_explain].item()\n",
        "        n_way = test_sampler.n_way\n",
        "\n",
        "        # --- Get Model Outputs & Explanation Info ---\n",
        "        predicted_label = -1\n",
        "        query_part_activation = None\n",
        "        class_part_profiles = {}\n",
        "        with torch.no_grad(): # Get predictions without grad\n",
        "             try:\n",
        "                 final_logits, _, explanation_info = model(\n",
        "                     support_images_dev, support_labels_dev, query_image_single_input, n_way\n",
        "                 )\n",
        "                 if final_logits is not None and final_logits.numel() > 0:\n",
        "                      predicted_label = torch.argmax(final_logits, dim=1).item()\n",
        "                      query_part_activation = explanation_info.get('query_part_activations') # [1, M]\n",
        "                      if query_part_activation is not None: query_part_activation = query_part_activation.squeeze(0).cpu() # [M]\n",
        "                      class_part_profiles = explanation_info.get('class_part_profiles', {})\n",
        "                 else: print(\"Warning: Model forward pass returned invalid logits.\")\n",
        "             except Exception as e:\n",
        "                  print(f\"Error during model forward pass for visualization: {e}\")\n",
        "                  import traceback; traceback.print_exc(); continue\n",
        "\n",
        "        print(f\"Explaining Query Img Idx: {query_idx_to_explain} | Pred: {predicted_label} | True: {true_label}\")\n",
        "\n",
        "        # --- Identify Support Images for Visualization ---\n",
        "        selected_support_indices = []\n",
        "        if predicted_label != -1:\n",
        "            support_indices_pred_class = torch.where(support_labels_cpu == predicted_label)[0]\n",
        "            # Select up to num_support_cams_per_class indices from the predicted class\n",
        "            selected_support_indices = support_indices_pred_class[:num_support_cams_per_class].tolist()\n",
        "            print(f\"  Will visualize CAMs for support indices: {selected_support_indices} (Predicted Class: {predicted_label})\")\n",
        "\n",
        "\n",
        "        # --- Create Figure (Adjust layout dynamically) ---\n",
        "        num_bar_charts = 0\n",
        "        if query_part_activation is not None: num_bar_charts += 1\n",
        "        if class_part_profiles.get(predicted_label) is not None: num_bar_charts += 1\n",
        "\n",
        "        can_do_cam = query_part_activation is not None and query_part_activation.numel() > 0\n",
        "        actual_top_k_parts = min(top_k_parts, query_part_activation.numel()) if can_do_cam else 0\n",
        "        num_query_cam_plots = actual_top_k_parts\n",
        "        num_support_cam_plots = len(selected_support_indices) * actual_top_k_parts # CAM for each selected support, for each top part\n",
        "\n",
        "        total_plots = 1 + num_bar_charts + num_query_cam_plots + num_support_cam_plots\n",
        "\n",
        "        if total_plots <= 1: N_COLS_VIS, N_ROWS_VIS = 1, 1\n",
        "        else:\n",
        "             # Adjust column count for better layout (e.g., up to 5 or 6 wide)\n",
        "             N_COLS_VIS = min(total_plots, 5 + num_support_cams_per_class)\n",
        "             N_ROWS_VIS = math.ceil(total_plots / N_COLS_VIS)\n",
        "\n",
        "        plt.figure(figsize=(max(12, 3.5 * N_COLS_VIS), 3.5 * N_ROWS_VIS)) # Adjusted figsize\n",
        "        plot_idx = 1\n",
        "        suptitle = f\"HEPN Expl {vis_idx+1}: Q Img {query_idx_to_explain} (Pred: {predicted_label}, True: {true_label})\"\n",
        "        if can_do_cam: suptitle += f\" | Top {actual_top_k_parts} Parts CAMs (Manual)\"\n",
        "        plt.suptitle(suptitle, fontsize=12, y=0.99)\n",
        "\n",
        "        # == Plot Query Image ==\n",
        "        ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "        query_img_np_viz = preprocess_image_for_viz(query_image_single_tensor)\n",
        "        if query_img_np_viz is not None: ax.imshow(query_img_np_viz)\n",
        "        ax.set_title(f\"Query Img {query_idx_to_explain}\\nTrue: {true_label} / Pred: {predicted_label}\")\n",
        "        ax.axis('off')\n",
        "\n",
        "        # == Plot Bar Charts ==\n",
        "        top_activated_part_indices = [] # Will store the indices of parts to visualize\n",
        "        if query_part_activation is not None and query_part_activation.numel() > 0:\n",
        "            ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "            sorted_vals, sorted_indices = torch.sort(query_part_activation, descending=True)\n",
        "            k_bar = min(10, query_part_activation.numel()) # Show top 10 in bar chart\n",
        "            if k_bar > 0:\n",
        "                bar_vals, bar_indices = sorted_vals[:k_bar], sorted_indices[:k_bar]\n",
        "                # Store the indices we'll actually use for CAM plots (top_k_parts)\n",
        "                top_activated_part_indices = sorted_indices[:actual_top_k_parts].tolist()\n",
        "                ax.barh(range(k_bar), bar_vals.flip(dims=[0]), tick_label=[f\"P {i}\" for i in bar_indices.flip(dims=[0]).tolist()])\n",
        "                ax.set_title(f\"Query Top {k_bar} Part Acts\"); ax.tick_params(axis='y', labelsize=8)\n",
        "            else: ax.axis('off'); ax.text(0.5, 0.5, \"No activations\", ha='center')\n",
        "        elif num_bar_charts > 0: ax=plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1; ax.axis('off'); ax.text(0.5, 0.5, \"No activation data\", ha='center')\n",
        "\n",
        "\n",
        "        pred_class_profile = class_part_profiles.get(predicted_label)\n",
        "        if pred_class_profile is not None and pred_class_profile.numel() > 0:\n",
        "            ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "            pred_class_profile_cpu = pred_class_profile.cpu()\n",
        "            sorted_prof_vals, sorted_prof_indices = torch.sort(pred_class_profile_cpu, descending=True)\n",
        "            k_prof_bar = min(10, pred_class_profile_cpu.numel())\n",
        "            if k_prof_bar > 0:\n",
        "                 bar_prof_vals, bar_prof_indices = sorted_prof_vals[:k_prof_bar], sorted_prof_indices[:k_prof_bar]\n",
        "                 ax.barh(range(k_prof_bar), bar_prof_vals.flip(dims=[0]), tick_label=[f\"P {i}\" for i in bar_prof_indices.flip(dims=[0]).tolist()])\n",
        "                 ax.set_title(f\"Pred Cls {predicted_label} Top {k_prof_bar} Profile\"); ax.tick_params(axis='y', labelsize=8)\n",
        "            else: ax.axis('off'); ax.text(0.5, 0.5, \"No profile values\", ha='center')\n",
        "        elif num_bar_charts > 1: ax=plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1; ax.axis('off'); ax.text(0.5, 0.5, \"No profile data\", ha='center')\n",
        "\n",
        "\n",
        "        # == Generate and Plot Manual Grad-CAM for QUERY Image ==\n",
        "        if can_do_cam and top_activated_part_indices:\n",
        "            print(f\"  Generating Manual Grad-CAM for QUERY image (idx {query_idx_to_explain}) top {len(top_activated_part_indices)} parts: {top_activated_part_indices}\")\n",
        "            input_image_np = query_img_np_viz # Use the HWC, 0-1 numpy image\n",
        "            if input_image_np is None: print(\"  Skipping Query CAMs as base image processing failed.\")\n",
        "            else:\n",
        "                for rank, part_idx in enumerate(top_activated_part_indices):\n",
        "                    if plot_idx > N_ROWS_VIS * N_COLS_VIS: print(f\"  Warning: Not enough plot slots for query CAMs...\"); break\n",
        "                    ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "\n",
        "                    grayscale_cam = calculate_gradcam_manual(model,\n",
        "                                                             target_layer_name,\n",
        "                                                             query_image_single_input, # Use the [1,C,H,W] tensor\n",
        "                                                             part_idx)\n",
        "\n",
        "                    if grayscale_cam is not None:\n",
        "                        visualization = show_cam_on_image(input_image_np, grayscale_cam, use_rgb=True)\n",
        "                        ax.imshow(visualization)\n",
        "                        ax.set_title(f\"Query P{part_idx} CAM (Rank {rank+1})\"); ax.axis('off')\n",
        "                    else:\n",
        "                         print(f\"  Failed to generate GradCAM for Query, Part {part_idx}.\")\n",
        "                         ax.set_title(f\"Query P{part_idx} CAM Fail\"); ax.axis('off'); continue\n",
        "\n",
        "        # == Generate and Plot Manual Grad-CAM for SUPPORT Images == ### NEW SECTION ###\n",
        "        if can_do_cam and top_activated_part_indices and len(selected_support_indices) > 0:\n",
        "            print(f\"  Generating Manual Grad-CAM for {len(selected_support_indices)} SUPPORT images (Pred Class {predicted_label}) using query's top {len(top_activated_part_indices)} parts.\")\n",
        "\n",
        "            for support_idx in selected_support_indices: # Iterate through actual indices\n",
        "                support_img_tensor = support_images_cpu[support_idx] # Get from CPU copy\n",
        "                support_img_np_viz = preprocess_image_for_viz(support_img_tensor)\n",
        "                support_img_input = support_img_tensor.unsqueeze(0).to(DEVICE) # Prep for model [1, C, H, W]\n",
        "\n",
        "                if support_img_np_viz is None:\n",
        "                    print(f\"  Skipping CAMs for Support Img {support_idx} as base image processing failed.\")\n",
        "                    # If strict grid needed, increment plot_idx by actual_top_k_parts here\n",
        "                    continue # Skip this support image\n",
        "\n",
        "                # Generate CAM for each top part for this support image\n",
        "                for rank, part_idx in enumerate(top_activated_part_indices):\n",
        "                    if plot_idx > N_ROWS_VIS * N_COLS_VIS:\n",
        "                        print(f\"  Warning: Ran out of plot slots before visualizing all support CAMs.\")\n",
        "                        break # Break inner loop (parts)\n",
        "                    ax = plt.subplot(N_ROWS_VIS, N_COLS_VIS, plot_idx); plot_idx+=1\n",
        "\n",
        "                    # Calculate Grad-CAM for the SUPPORT image\n",
        "                    grayscale_cam = calculate_gradcam_manual(\n",
        "                        model,\n",
        "                        target_layer_name,\n",
        "                        support_img_input, # Pass the support image tensor\n",
        "                        part_idx           # Use the part index from query's top parts\n",
        "                    )\n",
        "\n",
        "                    if grayscale_cam is not None:\n",
        "                        visualization = show_cam_on_image(support_img_np_viz, grayscale_cam, use_rgb=True)\n",
        "                        ax.imshow(visualization)\n",
        "                        # Better title indicating support image and part index\n",
        "                        ax.set_title(f\"Supp Idx {support_idx} - P{part_idx} CAM\\n(Q Part Rank {rank+1})\")\n",
        "                        ax.axis('off')\n",
        "                    else:\n",
        "                        print(f\"  Failed to generate GradCAM for Support Img {support_idx}, Part {part_idx}.\")\n",
        "                        ax.set_title(f\"Supp {support_idx} P{part_idx}\\nCAM Fail\"); ax.axis('off'); continue\n",
        "\n",
        "                if plot_idx > N_ROWS_VIS * N_COLS_VIS: break # Break outer loop (support images)\n",
        "\n",
        "\n",
        "        # --- Cleanup and Show ---\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout\n",
        "        plt.show()\n",
        "\n",
        "    # End of visualization loop\n",
        "\n",
        "\n",
        "# --- Main Execution (or wherever the visualization function is called) ---\n",
        "# Make sure the `trained_model` is loaded or trained before this call\n",
        "\n",
        "# Update the final call to include the new parameter:\n",
        "print(\"\\n5. Visualizing HEPN Explanation with Manual Grad-CAM (Query & Support)...\")\n",
        "if trained_model is not None:\n",
        "     visualize_explanation_hepn_manual_gradcam(\n",
        "          model=trained_model,\n",
        "          test_sampler=test_sampler,\n",
        "          target_layer_name=TARGET_LAYER_NAME,\n",
        "          num_explanations=5,\n",
        "          top_k_parts=3,                 # Visualize top 3 parts based on query activation\n",
        "          num_support_cams_per_class=3   # Show CAMs for first 2 support images of predicted class\n",
        "     )\n",
        "else:\n",
        "     print(\"ERROR: No valid model available for visualization.\")"
      ],
      "metadata": {
        "id": "QDX7HI9GL43V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}