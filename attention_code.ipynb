{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Improved Two-Level Prototypical Networks with Patch Explanation on CUB-200-2011\n",
        "\n",
        "Focuses on reducing overfitting without augmentation using:\n",
        "- Differential Learning Rates\n",
        "- Layer Freezing\n",
        "- Dropout\n",
        "- Label Smoothing\n",
        "- Gradient Clipping\n",
        "- Increased Episodes"
      ],
      "metadata": {
        "id": "EKz6JdTveIv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KIhrphsN7ORk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4_f1noD6zEd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "from tqdm.notebook import tqdm # Use standard tqdm if not in notebook\n",
        "import math\n",
        "import os\n",
        "from PIL import Image\n",
        "import copy # For deep copying models/optimizers state\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Configuration ---\n",
        "# -------------------------------------\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Dataset params\n",
        "DATA_DIR = '/content/drive/MyDrive/CUB_200_2011' #<---- CHECK YOUR PATH\n",
        "IMAGES_DIR = os.path.join(DATA_DIR, 'images')\n",
        "IMAGE_SIZE = 224\n",
        "# N_CLASSES_SUBSET # Will be determined dynamically now\n",
        "\n",
        "# Episode params\n",
        "N_WAY = 5          # Increase N_WAY for more challenging tasks if classes available\n",
        "K_SHOT = 5\n",
        "N_QUERY = 10\n",
        "# Increase significantly for meta-learning to see diverse tasks\n",
        "N_TRAIN_EPISODES = 1000\n",
        "N_TEST_EPISODES = 100\n",
        "\n",
        "# --- NEW: Fixed Meta-Split Configuration ---\n",
        "# Set USE_FIXED_SPLIT to True if you specifically want the 15/5 split\n",
        "# when exactly 20 classes are detected.\n",
        "USE_FIXED_SPLIT = True\n",
        "N_META_TRAIN_CLASSES_TARGET = 12\n",
        "N_META_TEST_CLASSES_TARGET = 8\n",
        "# The ratio below will be used as a fallback if USE_FIXED_SPLIT is False\n",
        "# or if the number of detected classes is not exactly 20.\n",
        "FALLBACK_META_TRAIN_RATIO = 0.7 # Keep the original ratio as fallback\n",
        "\n",
        "\n",
        "# Model params\n",
        "EMBEDDING_DIM = 256 # Feature embedding dimension\n",
        "PRETRAINED = True\n",
        "FREEZE_UNTIL_LAYER = \"layer3\" # Options: None, \"stem\", \"layer1\", \"layer2\", \"layer3\"\n",
        "\n",
        "# Training params (NEW: Differential LR & Stronger Regularization)\n",
        "LR_BACKBONE = 1e-5      # Lower LR for frozen/finetuned backbone\n",
        "LR_HEAD = 1e-4          # Higher LR for the new embedding layer\n",
        "WEIGHT_DECAY = 5e-4      # Increased weight decay\n",
        "LABEL_SMOOTHING = 0.1    # Use label smoothing\n",
        "GRADIENT_CLIP_NORM = 1.0 # Use gradient clipping\n",
        "\n",
        "# Visualization Params\n",
        "PATCH_SIZE_VIS = 35 # Approximate size of patch to draw\n",
        "\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm # Use auto tqdm\n",
        "\n",
        "# --- Make sure these are defined (or passed as arguments) ---\n",
        "# IMAGE_SIZE = 224 # Defined in config\n",
        "# K_SHOT = 5       # Defined in config\n",
        "# N_QUERY = 10     # Defined in config\n",
        "# N_WAY = 5        # Defined in config (can be adjusted later)\n",
        "# IMAGES_DIR = '...' # Defined in config\n",
        "\n",
        "def parse_cub_metadata(data_dir):\n",
        "    \"\"\"Reads CUB metadata files into pandas DataFrames.\"\"\"\n",
        "    images_path = os.path.join(data_dir, 'images.txt')\n",
        "    labels_path = os.path.join(data_dir, 'image_class_labels.txt')\n",
        "    split_path = os.path.join(data_dir, 'train_test_split.txt')\n",
        "    bbox_path = os.path.join(data_dir, 'bounding_boxes.txt')\n",
        "    classes_path = os.path.join(data_dir, 'classes.txt')\n",
        "\n",
        "    required_files = [images_path, labels_path, split_path, bbox_path, classes_path]\n",
        "    for f_path in required_files:\n",
        "        if not os.path.exists(f_path):\n",
        "            raise FileNotFoundError(f\"Metadata file not found: {f_path}.\")\n",
        "\n",
        "    df_images = pd.read_csv(images_path, sep=' ', names=['img_id', 'filepath'])\n",
        "    df_labels = pd.read_csv(labels_path, sep=' ', names=['img_id', 'class_id'])\n",
        "    df_split = pd.read_csv(split_path, sep=' ', names=['img_id', 'is_training'])\n",
        "    df_bboxes = pd.read_csv(bbox_path, sep=' ', names=['img_id', 'x', 'y', 'width', 'height'])\n",
        "    df_classes = pd.read_csv(classes_path, sep=' ', names=['class_id', 'class_name'])\n",
        "\n",
        "    df_labels['class_id'] = df_labels['class_id'] - 1 # 0-based\n",
        "    df = df_images.merge(df_labels, on='img_id')\n",
        "    df = df.merge(df_split, on='img_id')\n",
        "    df = df.merge(df_bboxes, on='img_id')\n",
        "    df['full_path'] = df['filepath'].apply(lambda x: os.path.join(IMAGES_DIR, x)) # Use global IMAGES_DIR\n",
        "    class_id_to_name = df_classes.set_index('class_id')['class_name'].to_dict()\n",
        "    class_id_to_name = {(k - 1): v for k, v in class_id_to_name.items()} # Adjust keys to 0-based\n",
        "    print(f\"Parsed metadata for {len(df)} total image entries across {df['class_id'].nunique()} original classes.\")\n",
        "    return df, class_id_to_name\n",
        "\n",
        "class CubDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for CUB-200-2011 with cropping.\"\"\"\n",
        "    def __init__(self, df_subset, transform=None):\n",
        "        self.df = df_subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_info = self.df.iloc[idx]\n",
        "        img_path = img_info['full_path']\n",
        "        label = img_info['subset_class_id'] # Use 0-based subset ID\n",
        "        bbox = (img_info['x'], img_info['y'], img_info['width'], img_info['height'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File not found {img_path}. Skipping.\")\n",
        "            return None, -1 # Return identifiable invalid data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            return None, -1 # Return identifiable invalid data\n",
        "\n",
        "        x, y, w, h = bbox\n",
        "        left, upper = int(np.floor(x)), int(np.floor(y))\n",
        "        right, lower = int(np.ceil(x + w)), int(np.ceil(y + h))\n",
        "        img_width, img_height = image.size\n",
        "        left, upper = max(0, left), max(0, upper)\n",
        "        right, lower = min(img_width, right), min(img_height, lower)\n",
        "\n",
        "        # Handle potential invalid bounding boxes (width/height=0 after clipping)\n",
        "        if right <= left or lower <= upper:\n",
        "            # Option 1: Use the full image if crop is invalid\n",
        "            # image = image\n",
        "            # Option 2: Skip this image (might reduce class sample size)\n",
        "            # print(f\"Warning: Invalid bbox resulting in zero-area crop for {img_path}. Skipping.\")\n",
        "            # return None, -1\n",
        "            # Option 3: Use a default crop or center crop (less ideal for CUB)\n",
        "            # Let's stick with using the full image if crop fails\n",
        "            pass # Keep original image if crop dims invalid\n",
        "        else:\n",
        "             image = image.crop((left, upper, right, lower))\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Ensure image is valid tensor before returning\n",
        "        if not isinstance(image, torch.Tensor):\n",
        "             print(f\"Warning: Transform did not produce a tensor for {img_path}. Skipping.\")\n",
        "             return None, -1\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "def prepare_cub_data_splits(data_dir,\n",
        "                              use_fixed_split=USE_FIXED_SPLIT, # Use global config\n",
        "                              n_meta_train_target=N_META_TRAIN_CLASSES_TARGET, # Use global config\n",
        "                              n_meta_test_target=N_META_TEST_CLASSES_TARGET, # Use global config\n",
        "                              fallback_ratio=FALLBACK_META_TRAIN_RATIO, # Use global config\n",
        "                              split_save_path=\"cub_meta_split.json\",\n",
        "                              force_resplit=False):\n",
        "    \"\"\"\n",
        "    Loads CUB data, detects available classes, prepares meta-splits\n",
        "    (using fixed numbers if possible and requested, otherwise ratio-based fallback),\n",
        "    and creates data lists for samplers.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to the CUB dataset root.\n",
        "        use_fixed_split (bool): Try to use fixed target numbers if total classes match.\n",
        "        n_meta_train_target (int): Target number of meta-train classes for fixed split.\n",
        "        n_meta_test_target (int): Target number of meta-test classes for fixed split.\n",
        "        fallback_ratio (float): Ratio for meta-training if fixed split not used.\n",
        "        split_save_path (str): Path to save/load the class split indices file.\n",
        "        force_resplit (bool): If True, ignore existing split file and create a new one.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (meta_train_data, meta_test_data, n_meta_train_classes, n_meta_test_classes) or (None, None, 0, 0) on failure.\n",
        "    \"\"\"\n",
        "    images_dir_local = os.path.join(data_dir, 'images')\n",
        "    if not os.path.isdir(images_dir_local):\n",
        "         raise FileNotFoundError(f\"Images directory not found: {images_dir_local}\")\n",
        "\n",
        "    df_all, class_id_to_name_map = parse_cub_metadata(data_dir)\n",
        "\n",
        "    # --- Detect Available Classes ---\n",
        "    available_folders = [d for d in os.listdir(images_dir_local) if os.path.isdir(os.path.join(images_dir_local, d))]\n",
        "    available_original_class_ids = []\n",
        "    for folder_name in available_folders:\n",
        "        try:\n",
        "            # Assumes folder names like \"001.Black_footed_Albatross\"\n",
        "            class_num_str = folder_name.split('.')[0]\n",
        "            original_id_one_based = int(class_num_str)\n",
        "            available_original_class_ids.append(original_id_one_based - 1) # Store 0-based IDs\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Could not parse class ID from folder name: {folder_name}. Skipping.\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error processing folder name {folder_name}: {e}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "    if not available_original_class_ids:\n",
        "        raise ValueError(\"No valid class folders found or parsed in images directory.\")\n",
        "\n",
        "    # Filter df_all to only include images from detected classes BEFORE getting unique IDs\n",
        "    df_all_detected = df_all[df_all['class_id'].isin(available_original_class_ids)].copy()\n",
        "    if df_all_detected.empty:\n",
        "         raise ValueError(\"No images found corresponding to the detected class folders.\")\n",
        "\n",
        "    # Get the unique, sorted list of class IDs *that actually have images*\n",
        "    final_available_class_ids = sorted(df_all_detected['class_id'].unique().tolist())\n",
        "    N_CLASSES_TOTAL_DETECTED = len(final_available_class_ids)\n",
        "\n",
        "\n",
        "    print(f\"\\nDetected {N_CLASSES_TOTAL_DETECTED} classes with available images.\")\n",
        "    print(\"Sample Detected Classes:\")\n",
        "    for i, cid in enumerate(final_available_class_ids[:min(5, N_CLASSES_TOTAL_DETECTED)]):\n",
        "         print(f\"  ID {cid}: {class_id_to_name_map.get(cid, 'Unknown Name')}\")\n",
        "    if N_CLASSES_TOTAL_DETECTED > 5: print(\"  ...\")\n",
        "\n",
        "\n",
        "    # --- Load or Create Meta-Split ---\n",
        "    meta_train_subset_indices = None\n",
        "    meta_test_subset_indices = None\n",
        "    target_total_classes = n_meta_train_target + n_meta_test_target\n",
        "\n",
        "    if os.path.exists(split_save_path) and not force_resplit:\n",
        "        print(f\"Attempting to load existing class split from: {split_save_path}\")\n",
        "        try:\n",
        "            with open(split_save_path, 'r') as f:\n",
        "                split_data = json.load(f)\n",
        "            # Basic validation\n",
        "            if ('meta_train_indices' in split_data and 'meta_test_indices' in split_data and\n",
        "                isinstance(split_data['meta_train_indices'], list) and\n",
        "                isinstance(split_data['meta_test_indices'], list)):\n",
        "\n",
        "                loaded_train_indices = split_data['meta_train_indices']\n",
        "                loaded_test_indices = split_data['meta_test_indices']\n",
        "\n",
        "                # Map loaded indices (which are subset indices) back to original class IDs\n",
        "                # This requires knowing how the split was originally created, which is tricky.\n",
        "                # Let's instead check if the *number* of classes in the split matches expectations.\n",
        "                # This isn't foolproof but better than nothing.\n",
        "                expected_num_train = n_meta_train_target if use_fixed_split and N_CLASSES_TOTAL_DETECTED == target_total_classes else int(N_CLASSES_TOTAL_DETECTED * fallback_ratio)\n",
        "                expected_num_test = n_meta_test_target if use_fixed_split and N_CLASSES_TOTAL_DETECTED == target_total_classes else N_CLASSES_TOTAL_DETECTED - expected_num_train\n",
        "\n",
        "                # Check for overlap\n",
        "                if set(loaded_train_indices).intersection(set(loaded_test_indices)):\n",
        "                     print(\"Warning: Loaded split has overlapping train/test indices. Will create new split.\")\n",
        "                # Check if counts broadly match expectation (loosely)\n",
        "                elif len(loaded_train_indices) + len(loaded_test_indices) != N_CLASSES_TOTAL_DETECTED:\n",
        "                     print(f\"Warning: Loaded split has {len(loaded_train_indices)+len(loaded_test_indices)} total indices, but {N_CLASSES_TOTAL_DETECTED} classes detected now. Will create new split.\")\n",
        "                else:\n",
        "                    # If counts match detected total and no overlap, assume it's valid for now\n",
        "                    # The actual classes inside might differ run-to-run if detection changes slightly,\n",
        "                    # but using the saved indices maintains consistency *for those indices*.\n",
        "                    meta_train_subset_indices = loaded_train_indices\n",
        "                    meta_test_subset_indices = loaded_test_indices\n",
        "                    print(\"Successfully loaded existing split. Using saved class indices.\")\n",
        "            else:\n",
        "                print(f\"Warning: Invalid format in {split_save_path}. Will create new split.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading split file {split_save_path}: {e}. Will create new split.\")\n",
        "\n",
        "    # If split wasn't loaded, create and save a new one\n",
        "    if meta_train_subset_indices is None or meta_test_subset_indices is None:\n",
        "        print(\"Creating new meta-train/test class split...\")\n",
        "        split_comment = \"\"\n",
        "\n",
        "        # --- CORE CHANGE: Decide on split numbers ---\n",
        "        if use_fixed_split and N_CLASSES_TOTAL_DETECTED == target_total_classes:\n",
        "            n_meta_train_actual = n_meta_train_target\n",
        "            n_meta_test_actual = n_meta_test_target\n",
        "            print(f\"Using fixed split: {n_meta_train_actual} meta-train / {n_meta_test_actual} meta-test classes.\")\n",
        "            split_comment = f'Fixed {n_meta_train_actual}/{n_meta_test_actual} split of {N_CLASSES_TOTAL_DETECTED} detected classes.'\n",
        "\n",
        "        else:\n",
        "            if use_fixed_split: # Only print warning if fixed was intended but not possible\n",
        "                 print(f\"WARNING: Cannot use fixed {n_meta_train_target}/{n_meta_test_target} split because {N_CLASSES_TOTAL_DETECTED} classes were detected (expected {target_total_classes}).\")\n",
        "            print(f\"Using fallback ratio-based split ({fallback_ratio*100:.1f}% train).\")\n",
        "            n_meta_train_actual = int(N_CLASSES_TOTAL_DETECTED * fallback_ratio)\n",
        "            n_meta_test_actual = N_CLASSES_TOTAL_DETECTED - n_meta_train_actual\n",
        "            split_comment = f'Ratio-based ({fallback_ratio*100:.1f}%) split of {N_CLASSES_TOTAL_DETECTED} detected classes.'\n",
        "\n",
        "        # --- Ensure split is possible ---\n",
        "        if n_meta_train_actual <= 0 or n_meta_test_actual <= 0:\n",
        "            raise ValueError(f\"Cannot split {N_CLASSES_TOTAL_DETECTED} classes into train/test ({n_meta_train_actual}/{n_meta_test_actual}). Need more classes or adjust split criteria.\")\n",
        "        # Ensure N_WAY is feasible even for the smaller split part\n",
        "        global N_WAY # Allow modification\n",
        "        if N_WAY > n_meta_train_actual or N_WAY > n_meta_test_actual:\n",
        "             min_possible_way = min(n_meta_train_actual, n_meta_test_actual)\n",
        "             if min_possible_way > 0:\n",
        "                 print(f\"WARNING: N_WAY ({N_WAY}) is too large for the planned split ({n_meta_train_actual} train, {n_meta_test_actual} test classes). Reducing N_WAY to {min_possible_way}.\")\n",
        "                 N_WAY = min_possible_way\n",
        "             else:\n",
        "                  raise ValueError(f\"Cannot proceed: Split resulted in {n_meta_train_actual} train / {n_meta_test_actual} test classes, which is too few for N_WAY={N_WAY}.\")\n",
        "\n",
        "\n",
        "        # --- Perform the split ---\n",
        "        # We split the *original class IDs* that were detected\n",
        "        current_indices = list(range(N_CLASSES_TOTAL_DETECTED)) # Indices corresponding to final_available_class_ids\n",
        "        random.shuffle(current_indices)\n",
        "\n",
        "        meta_train_subset_indices = sorted(current_indices[:n_meta_train_actual])\n",
        "        meta_test_subset_indices = sorted(current_indices[n_meta_train_actual:])\n",
        "\n",
        "        # --- Save the new split (using the subset indices 0..N-1) ---\n",
        "        try:\n",
        "            split_data_to_save = {\n",
        "                'meta_train_indices': meta_train_subset_indices, # Save indices 0..N-1\n",
        "                'meta_test_indices': meta_test_subset_indices,   # Save indices 0..N-1\n",
        "                'comment': split_comment,\n",
        "                'original_class_ids_used': final_available_class_ids # Store which original IDs these indices correspond to\n",
        "            }\n",
        "            with open(split_save_path, 'w') as f:\n",
        "                json.dump(split_data_to_save, f, indent=4)\n",
        "            print(f\"Saved new class split to: {split_save_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving new split file {split_save_path}: {e}\")\n",
        "\n",
        "\n",
        "    # Now we have the definitive meta_train/test_subset_indices (relative to final_available_class_ids)\n",
        "    n_meta_train_final = len(meta_train_subset_indices)\n",
        "    n_meta_test_final = len(meta_test_subset_indices)\n",
        "    print(f\"Using finalized split: {n_meta_train_final} meta-train classes, {n_meta_test_final} meta-test classes.\")\n",
        "\n",
        "    # --- Map subset indices (0..N-1) back to original CUB class IDs ---\n",
        "    meta_train_original_class_ids = [final_available_class_ids[i] for i in meta_train_subset_indices]\n",
        "    meta_test_original_class_ids = [final_available_class_ids[i] for i in meta_test_subset_indices]\n",
        "\n",
        "    # --- Create the final DataFrame subsets based on ORIGINAL Class IDs ---\n",
        "    df_meta_train = df_all_detected[df_all_detected['class_id'].isin(meta_train_original_class_ids)].copy()\n",
        "    df_meta_test = df_all_detected[df_all_detected['class_id'].isin(meta_test_original_class_ids)].copy()\n",
        "\n",
        "    # --- Create new SUBSET class IDs (0 to num_train-1) and (0 to num_test-1) for each split ---\n",
        "    # This is important for the EpisodeSampler which expects contiguous labels 0 to N_WAY-1\n",
        "    train_map = {orig_id: new_id for new_id, orig_id in enumerate(sorted(meta_train_original_class_ids))}\n",
        "    test_map = {orig_id: new_id for new_id, orig_id in enumerate(sorted(meta_test_original_class_ids))}\n",
        "\n",
        "    df_meta_train['subset_class_id'] = df_meta_train['class_id'].map(train_map)\n",
        "    df_meta_test['subset_class_id'] = df_meta_test['class_id'].map(test_map)\n",
        "\n",
        "    # --- Define Transforms ---\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    # Separate CUB's train/test split *within* our meta-train/test class splits\n",
        "    df_meta_train_cub_train = df_meta_train[df_meta_train['is_training'] == 1]\n",
        "    df_meta_test_cub_test = df_meta_test[df_meta_test['is_training'] == 0]\n",
        "\n",
        "    # --- Helper to load images for meta-datasets ---\n",
        "    def load_data_for_split(target_original_ids, df_source, subset_map, split_name):\n",
        "        data = []\n",
        "        # Use the subset_map to iterate through 0..N-1 required by sampler\n",
        "        pbar = tqdm(range(len(target_original_ids)), desc=f\"Processing Meta-{split_name} Classes\")\n",
        "        original_ids_in_order = sorted(target_original_ids) # Ensure consistent order\n",
        "\n",
        "        for subset_class_id in pbar: # Iterate 0..N-1\n",
        "            original_class_id = original_ids_in_order[subset_class_id]\n",
        "            pbar.set_postfix({\"Orig ClassID\": original_class_id})\n",
        "\n",
        "            # Filter the source DataFrame for this original class ID\n",
        "            class_df = df_source[df_source['class_id'] == original_class_id]\n",
        "\n",
        "            if not class_df.empty:\n",
        "                 # Pass the DataFrame already filtered for this class, ensuring correct 'subset_class_id'\n",
        "                temp_dataset = CubDataset(class_df, transform=transform)\n",
        "                # Collect images, ensuring they are not None\n",
        "                images = []\n",
        "                labels = []\n",
        "                for i in range(len(temp_dataset)):\n",
        "                    img, lbl = temp_dataset[i]\n",
        "                    if img is not None:\n",
        "                        # Ensure the label is the subset_class_id (0..N-1)\n",
        "                        assert lbl == subset_class_id, f\"Label mismatch! Expected {subset_class_id}, got {lbl} for original ID {original_class_id}\"\n",
        "                        images.append(img)\n",
        "                        labels.append(lbl) # Store the correct subset label\n",
        "\n",
        "                if images:\n",
        "                    data.append((subset_class_id, images)) # Store with the SUBSET class ID\n",
        "\n",
        "        # Validation checks (using potentially reduced N_WAY)\n",
        "        min_samples_needed = K_SHOT + N_QUERY\n",
        "        valid_classes_count = 0\n",
        "        problematic_classes = []\n",
        "        for subset_id, imgs in data:\n",
        "             if len(imgs) >= min_samples_needed:\n",
        "                 valid_classes_count += 1\n",
        "             else:\n",
        "                 # Find original ID for reporting\n",
        "                 orig_id_for_report = original_ids_in_order[subset_id]\n",
        "                 problematic_classes.append(f\"ID {orig_id_for_report} (subset {subset_id}): {len(imgs)} samples\")\n",
        "\n",
        "        if problematic_classes:\n",
        "             print(f\"\\n*** WARNING ({split_name}): {len(problematic_classes)}/{len(data)} classes have < {min_samples_needed} samples (need replacement). Examples:\")\n",
        "             for problem in problematic_classes[:5]: print(f\"  - {problem}\")\n",
        "             if len(problematic_classes) > 5: print(\"  ...\")\n",
        "\n",
        "        if valid_classes_count < N_WAY: # Use potentially reduced N_WAY\n",
        "             print(f\"*** CRITICAL WARNING ({split_name}): Only {valid_classes_count} classes have enough samples ({min_samples_needed} required). N_WAY ({N_WAY}) might be impossible without replacement during sampling.\")\n",
        "\n",
        "        # Sort data by subset_class_id to ensure consistency\n",
        "        data.sort(key=lambda x: x[0])\n",
        "        return data\n",
        "\n",
        "    print(\"\\nBuilding meta-train data (using CUB train split images)...\")\n",
        "    # Use the CUB training images for meta-training classes\n",
        "    meta_train_data = load_data_for_split(meta_train_original_class_ids, df_meta_train_cub_train, train_map, \"Train\")\n",
        "\n",
        "    print(\"\\nBuilding meta-test data (using CUB test split images)...\")\n",
        "    # Use the CUB testing images for meta-testing classes\n",
        "    meta_test_data = load_data_for_split(meta_test_original_class_ids, df_meta_test_cub_test, test_map, \"Test\")\n",
        "\n",
        "    print(f\"\\nFinal classes available for meta-train sampler: {len(meta_train_data)}\")\n",
        "    print(f\"Final classes available for meta-test sampler: {len(meta_test_data)}\")\n",
        "\n",
        "    if not meta_train_data or not meta_test_data:\n",
        "        print(\"\\n*** ERROR: Empty meta_train_data or meta_test_data after processing. Check data integrity/paths/filtering. ***\")\n",
        "        return None, None, 0, 0\n",
        "\n",
        "    # Return the data lists AND the actual number of classes in each split\n",
        "    return meta_train_data, meta_test_data, n_meta_train_final, n_meta_test_final\n",
        "\n",
        "# --- Episode Sampler (Largely Unchanged, but relies on correct input format) ---\n",
        "class EpisodeSampler:\n",
        "    \"\"\"Samples episodes for N-way K-shot learning.\"\"\"\n",
        "    def __init__(self, meta_data, n_way, k_shot, n_query):\n",
        "        self.meta_data = meta_data # Expects list of [(subset_class_id_0, [img_tensors]), (subset_class_id_1, [img_tensors]), ...]\n",
        "        if not meta_data:\n",
        "            raise ValueError(\"Meta data for sampler cannot be empty or None\")\n",
        "\n",
        "        self.num_classes = len(meta_data) # Number of classes available in this split\n",
        "\n",
        "        # Validate n_way against the *actual* number of classes provided\n",
        "        if n_way <= 0 :\n",
        "             raise ValueError(f\"N_WAY ({n_way}) must be positive.\")\n",
        "        if n_way > self.num_classes:\n",
        "            raise ValueError(f\"N_WAY ({n_way}) cannot be greater than the number of classes available in this meta-split ({self.num_classes}).\")\n",
        "\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.n_query = n_query\n",
        "        self.min_samples_per_class = k_shot + n_query\n",
        "\n",
        "        # Pre-check if sampling is even possible without replacement for all classes\n",
        "        classes_with_enough_samples = 0\n",
        "        for subset_class_id, images in self.meta_data:\n",
        "            if len(images) >= self.min_samples_per_class:\n",
        "                classes_with_enough_samples += 1\n",
        "        if classes_with_enough_samples < self.n_way:\n",
        "             print(f\"Sampler Warning: Only {classes_with_enough_samples} classes have the required {self.min_samples_per_class} samples. N_WAY is {self.n_way}. Sampling will require replacement for some classes.\")\n",
        "\n",
        "\n",
        "    def sample(self):\n",
        "        support_imgs, support_lbls, query_imgs, query_lbls = [], [], [], []\n",
        "\n",
        "        # Sample N_WAY class *indices* from the available classes in meta_data\n",
        "        try:\n",
        "            # These are indices into the self.meta_data list (0 to self.num_classes - 1)\n",
        "            sampled_meta_indices = random.sample(range(self.num_classes), self.n_way)\n",
        "        except ValueError as e:\n",
        "            # This should have been caught by the __init__ check, but double-check\n",
        "            print(f\"Error sampling class indices: Need {self.n_way}-way, but only {self.num_classes} classes available.\")\n",
        "            raise e\n",
        "\n",
        "        # Iterate through the *sampled* classes using their index in meta_data\n",
        "        for local_label, meta_idx in enumerate(sampled_meta_indices):\n",
        "            # Retrieve the actual subset_class_id and images for this sampled class\n",
        "            # The subset_class_id stored in meta_data[meta_idx][0] should match meta_idx if data was sorted, but retrieve image list anyway\n",
        "            _, images = self.meta_data[meta_idx]\n",
        "            n_available = len(images)\n",
        "            n_needed = self.k_shot + self.n_query\n",
        "\n",
        "            # Decide whether to sample with replacement for this specific class\n",
        "            use_replacement = n_available < n_needed\n",
        "            if use_replacement:\n",
        "                 # print(f\"Debug: Using replacement for class (meta_idx {meta_idx}), needed {n_needed}, available {n_available}\")\n",
        "                 pass # Can add print for debugging\n",
        "\n",
        "            # Sample indices *from the image list of this class*\n",
        "            try:\n",
        "                indices = random.choices(range(n_available), k=n_needed) if use_replacement else random.sample(range(n_available), n_needed)\n",
        "            except ValueError as e:\n",
        "                 print(f\"Error sampling images for class (meta_idx {meta_idx}): Needed {n_needed}, available {n_available}, Replacement={use_replacement}\")\n",
        "                 # This likely means n_available is 0, which should ideally be filtered earlier\n",
        "                 # For robustness, maybe skip this class in the episode? Or raise error? Let's raise.\n",
        "                 raise ValueError(f\"Cannot sample {n_needed} images from class (meta_idx {meta_idx}) with only {n_available} available.\") from e\n",
        "\n",
        "\n",
        "            # Assign images to support/query sets using the sampled image indices\n",
        "            support_imgs.extend([images[i] for i in indices[:self.k_shot]])\n",
        "            support_lbls.extend([local_label] * self.k_shot) # Use the local label (0 to N_WAY-1)\n",
        "            query_imgs.extend([images[i] for i in indices[self.k_shot:]])\n",
        "            query_lbls.extend([local_label] * self.n_query)   # Use the local label (0 to N_WAY-1)\n",
        "\n",
        "        # --- Batch and Shuffle ---\n",
        "        # Check if any tensors were actually collected\n",
        "        if not support_imgs or not query_imgs:\n",
        "             # This could happen if all sampled classes had errors or no images\n",
        "             raise RuntimeError(\"Failed to collect any support or query images for the episode.\")\n",
        "\n",
        "        try:\n",
        "            support_imgs = torch.stack(support_imgs)\n",
        "            support_lbls = torch.LongTensor(support_lbls)\n",
        "            query_imgs = torch.stack(query_imgs)\n",
        "            query_lbls = torch.LongTensor(query_lbls)\n",
        "        except Exception as e:\n",
        "            print(f\"Error stacking tensors. Support length: {len(support_imgs)}, Query length: {len(query_imgs)}\")\n",
        "            # Optionally print shapes of individual tensors if they fail to stack\n",
        "            # for i, img in enumerate(support_imgs): print(f\"Support img {i} shape: {img.shape}\")\n",
        "            # for i, img in enumerate(query_imgs): print(f\"Query img {i} shape: {img.shape}\")\n",
        "            raise e\n",
        "\n",
        "\n",
        "        # Shuffle query set only (support set order doesn't usually matter for prototypes)\n",
        "        perm = torch.randperm(len(query_lbls))\n",
        "        query_imgs, query_lbls = query_imgs[perm], query_lbls[perm]\n",
        "\n",
        "        return support_imgs, support_lbls, query_imgs, query_lbls"
      ],
      "metadata": {
        "id": "1e5n6voheVSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Model Architecture (Improved Encoder) ---\n",
        "# -------------------------------------\n",
        "class ResNetEncoderWithDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder based on pretrained ResNet18 with layer freezing and dropout.\n",
        "    Provides access to both convolutional features and final embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=EMBEDDING_DIM, pretrained=True, freeze_until=None, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
        "\n",
        "        # Backbone Feature Extraction Layers\n",
        "        self.stem = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
        "        self.layer1 = resnet.layer1\n",
        "        self.layer2 = resnet.layer2\n",
        "        self.layer3 = resnet.layer3\n",
        "        self.layer4 = resnet.layer4\n",
        "\n",
        "        # Feature extractor combines these layers\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            self.stem, self.layer1, self.layer2, self.layer3, self.layer4\n",
        "        )\n",
        "\n",
        "        # Freeze layers based on configuration\n",
        "        if freeze_until:\n",
        "            self._freeze_layers(freeze_until)\n",
        "\n",
        "        # Embedding Head\n",
        "        resnet_out_dim = resnet.fc.in_features # 512 for ResNet18\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate) # Add dropout before final layer\n",
        "        self.embedding_layer = nn.Linear(resnet_out_dim, embedding_dim)\n",
        "\n",
        "        self.embedding_head = nn.Sequential(\n",
        "            self.global_pool,\n",
        "            self.flatten,\n",
        "            self.dropout,\n",
        "            self.embedding_layer\n",
        "        )\n",
        "\n",
        "        # Calculate total stride (typically 32 for ResNet18)\n",
        "        self.total_stride = 32 # Assuming standard ResNet strides\n",
        "\n",
        "    def _freeze_layers(self, freeze_until):\n",
        "        print(f\"Freezing ResNet layers up to and including: {freeze_until}\")\n",
        "        target_modules = []\n",
        "        if freeze_until == \"stem\":\n",
        "            target_modules.append(self.stem)\n",
        "        elif freeze_until == \"layer1\":\n",
        "            target_modules.extend([self.stem, self.layer1])\n",
        "        elif freeze_until == \"layer2\":\n",
        "            target_modules.extend([self.stem, self.layer1, self.layer2])\n",
        "        elif freeze_until == \"layer3\":\n",
        "            target_modules.extend([self.stem, self.layer1, self.layer2, self.layer3])\n",
        "        # Note: We don't explicitly handle layer4 here for freezing based on common practice.\n",
        "        # layer4 and embedding_head should generally remain trainable for adaptation.\n",
        "\n",
        "        for module in target_modules:\n",
        "            for param in module.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # --- Verification ---\n",
        "        print(\"Trainable status check (at end of _freeze_layers):\")\n",
        "        # Check only modules DEFINED before or during freezing\n",
        "        defined_modules_to_check = [\n",
        "            ('stem', self.stem),\n",
        "            ('layer1', self.layer1),\n",
        "            ('layer2', self.layer2),\n",
        "            ('layer3', self.layer3),\n",
        "            ('layer4', self.layer4) # Layer 4 is defined, just maybe not frozen\n",
        "        ]\n",
        "        for name, module in defined_modules_to_check:\n",
        "             try: # Use try-except in case a layer is unexpectedly missing\n",
        "                 is_trainable = any(p.requires_grad for p in module.parameters())\n",
        "                 print(f\"  {name}: {'Trainable' if is_trainable else 'Frozen'}\")\n",
        "             except AttributeError:\n",
        "                 print(f\"  {name}: Not found (AttributeError during check)\")\n",
        "        # We will check embedding_head status later, after it's fully initialized\n",
        "        print(\"(Embedding head status check deferred)\")\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_features = self.feature_extractor(x) # e.g., [B, 512, H/32, W/32]\n",
        "        embedding = self.embedding_head(conv_features) # -> [B, embedding_dim]\n",
        "        return conv_features, embedding\n",
        "\n",
        "    def get_trainable_parameters(self):\n",
        "        \"\"\" Helper to get parameters for differential learning rates \"\"\"\n",
        "        backbone_params = []\n",
        "        head_params = []\n",
        "\n",
        "        # Explicitly identify parameters of the embedding head layers by their attribute names\n",
        "        head_param_names = set()\n",
        "        for param_name, _ in self.embedding_layer.named_parameters():\n",
        "            head_param_names.add(f\"embedding_layer.{param_name}\")\n",
        "        # Add parameters from other head components if they have any (dropout usually doesn't)\n",
        "        # e.g., if you added another Linear or Conv2d layer to the head\n",
        "\n",
        "        print(f\"DEBUG: Identified head parameter base names: {head_param_names}\")\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            # Check if parameter requires gradient (might be frozen)\n",
        "            if not param.requires_grad:\n",
        "                continue\n",
        "\n",
        "            is_head_param = False\n",
        "            # Check if the parameter name corresponds to one of the layers in the head\n",
        "            # Primarily check the final linear layer which always has trainable parameters\n",
        "            if name.startswith(\"embedding_layer.\"): # Direct check is simplest\n",
        "                is_head_param = True\n",
        "            # Optional: add checks for other head layers if they have parameters\n",
        "            # elif name.startswith(\"some_other_head_layer.\"):\n",
        "            #    is_head_param = True\n",
        "\n",
        "\n",
        "            if is_head_param:\n",
        "                 print(f\"DEBUG: Assigning '{name}' to HEAD parameters.\")\n",
        "                 head_params.append(param)\n",
        "            else:\n",
        "                 # Assign remaining trainable params (from unfrozen backbone layers like layer4) to backbone group\n",
        "                 print(f\"DEBUG: Assigning '{name}' to BACKBONE parameters.\")\n",
        "                 backbone_params.append(param)\n",
        "\n",
        "        print(f\"Differential LR setup: {len(backbone_params)} backbone params, {len(head_params)} head params.\")\n",
        "        if not head_params:\n",
        "             print(\"Warning: No parameters identified for the embedding head!\")\n",
        "        # It's okay if backbone_params is small if many layers are frozen\n",
        "        # if not backbone_params:\n",
        "        #     print(\"Warning: No parameters identified for the backbone (is everything frozen?)\")\n",
        "\n",
        "        return backbone_params, head_params\n",
        "\n",
        "\n",
        "# --- PrototypicalNet (Minor Adjustments for Stability/Clarity) ---\n",
        "class ExplainablePrototypicalNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Main model using the improved ResNet encoder with dropout/freezing.\n",
        "    Implements Level 1 Classification and Level 2 Patch Explanation.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def _calculate_prototypes(self, support_embeddings, support_labels, n_way):\n",
        "        # Add small epsilon for numerical stability if needed, though mean is usually fine\n",
        "        prototypes = torch.zeros(n_way, support_embeddings.size(1), device=support_embeddings.device)\n",
        "        for c in range(n_way):\n",
        "            class_embeddings = support_embeddings[support_labels == c]\n",
        "            if class_embeddings.size(0) > 0:\n",
        "                prototypes[c] = class_embeddings.mean(dim=0)\n",
        "            # Else: prototype remains zero, distance will be high (correct behaviour)\n",
        "        return prototypes\n",
        "\n",
        "    def _compute_distances(self, query_embeddings, prototypes):\n",
        "         # Calculate squared Euclidean distance efficiently\n",
        "        return torch.cdist(query_embeddings, prototypes)**2\n",
        "\n",
        "    def level1_classify(self, support_embeddings, support_labels, query_embeddings, n_way):\n",
        "        prototypes = self._calculate_prototypes(support_embeddings, support_labels, n_way)\n",
        "        # distances shape: [n_query, n_way]\n",
        "        distances = self._compute_distances(query_embeddings, prototypes)\n",
        "        logits = -distances # Lower distance = higher logit\n",
        "        return logits\n",
        "\n",
        "    def level2_explain_per_support_image_topk(self, query_conv_features, support_conv_features_list,predicted_class_support_indices, top_k=3):\n",
        "        explanation_results_per_support = {}\n",
        "        if query_conv_features.size(0) != 1: return explanation_results_per_support\n",
        "        if not predicted_class_support_indices: return explanation_results_per_support\n",
        "\n",
        "        B, D, H, W = query_conv_features.shape\n",
        "        N_PATCHES_Q = H * W\n",
        "        if N_PATCHES_Q <= 0: return explanation_results_per_support\n",
        "\n",
        "        query_patches = query_conv_features.permute(0, 2, 3, 1).reshape(N_PATCHES_Q, D)\n",
        "        query_patches_clean = torch.nan_to_num(query_patches, nan=0.0) # Basic cleaning\n",
        "\n",
        "        scale_factor = math.sqrt(D) if D > 0 else 1.0\n",
        "\n",
        "        for support_global_idx in predicted_class_support_indices:\n",
        "            support_feat_single = support_conv_features_list[support_global_idx].unsqueeze(0) # [1, D, H, W]\n",
        "            _, _, Hs, Ws = support_feat_single.shape\n",
        "            N_PATCHES_S = Hs * Ws\n",
        "            if N_PATCHES_S <= 0: continue # Skip invalid support feature map\n",
        "\n",
        "            support_patches_single_img = support_feat_single.permute(0, 2, 3, 1).reshape(N_PATCHES_S, D)\n",
        "            support_patches_single_clean = torch.nan_to_num(support_patches_single_img, nan=0.0)\n",
        "\n",
        "            top_k_results_for_support = []\n",
        "            try:\n",
        "                # Calculate pairwise similarity (using attention scores here)\n",
        "                attn_scores = torch.matmul(query_patches_clean, support_patches_single_clean.t()) / scale_factor\n",
        "                # Shape: [N_PATCHES_Q, N_PATCHES_S]\n",
        "\n",
        "                if torch.isnan(attn_scores).any() or torch.isinf(attn_scores).any():\n",
        "                     print(f\"Warning: NaN/Inf in scores for support img {support_global_idx}. Clamping.\")\n",
        "                     attn_scores = torch.nan_to_num(attn_scores, nan=-1e9) # Assign low score\n",
        "\n",
        "                # Get top-k scores and their flat indices\n",
        "                num_elements = N_PATCHES_Q * N_PATCHES_S\n",
        "                actual_top_k = min(top_k, num_elements) # Ensure top_k doesn't exceed total elements\n",
        "                if actual_top_k <= 0: continue # Skip if no patches\n",
        "\n",
        "                top_k_scores, top_k_flat_indices = torch.topk(attn_scores.flatten(), k=actual_top_k, dim=0)\n",
        "\n",
        "                # Convert flat indices back to (query_idx, support_idx)\n",
        "                for i in range(actual_top_k):\n",
        "                    flat_idx = top_k_flat_indices[i].item()\n",
        "                    q_idx = flat_idx // N_PATCHES_S\n",
        "                    s_idx = flat_idx % N_PATCHES_S\n",
        "                    score = top_k_scores[i].item()\n",
        "                    top_k_results_for_support.append((q_idx, s_idx, score))\n",
        "\n",
        "                explanation_results_per_support[support_global_idx] = top_k_results_for_support\n",
        "\n",
        "            except Exception as e:\n",
        "                 print(f\"Error during Top-K explanation for support img {support_global_idx}: {e}\")\n",
        "                 continue # Skip this support image on error\n",
        "\n",
        "        return explanation_results_per_support\n",
        "\n",
        "    def forward(self, support_images, support_labels, query_images, n_way):\n",
        "        n_support = support_images.size(0)\n",
        "        n_query = query_images.size(0)\n",
        "        all_images = torch.cat([support_images, query_images], dim=0)\n",
        "\n",
        "        # Get features and embeddings\n",
        "        # Use try-except to catch potential errors during forward pass (e.g. OOM)\n",
        "        try:\n",
        "            all_conv_features, all_embeddings = self.encoder(all_images)\n",
        "        except Exception as e:\n",
        "            print(f\"Error during encoder forward pass: {e}\")\n",
        "             # Return dummy values or re-raise, depends on how you want to handle upstream\n",
        "            dummy_logits = torch.zeros(n_query, n_way).to(all_images.device)\n",
        "            dummy_q_conv = torch.zeros(n_query, EMBEDDING_DIM, 1, 1).to(all_images.device) # Adjust D,H,W if needed\n",
        "            dummy_s_conv = torch.zeros(n_support, EMBEDDING_DIM, 1, 1).to(all_images.device)\n",
        "            return dummy_logits, dummy_q_conv, dummy_s_conv\n",
        "\n",
        "        # Split back into support and query\n",
        "        support_embeddings = all_embeddings[:n_support]\n",
        "        query_embeddings = all_embeddings[n_support:]\n",
        "        support_conv_features = all_conv_features[:n_support]\n",
        "        query_conv_features = all_conv_features[n_support:]\n",
        "\n",
        "        # Level 1 classification\n",
        "        logits = self.level1_classify(support_embeddings, support_labels, query_embeddings, n_way)\n",
        "\n",
        "        return logits, query_conv_features, support_conv_features"
      ],
      "metadata": {
        "id": "vg3P9AE0em7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Training & Evaluation (Improved) ---\n",
        "# -------------------------------------\n",
        "\n",
        "# Use CrossEntropyLoss with Label Smoothing\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
        "\n",
        "def train_step(model, optimizer, loss_fn, support_images, support_labels, query_images, query_labels, n_way, gradient_clip_norm):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    support_images = support_images.to(DEVICE)\n",
        "    support_labels = support_labels.to(DEVICE)\n",
        "    query_images = query_images.to(DEVICE)\n",
        "    query_labels = query_labels.to(DEVICE)\n",
        "\n",
        "    logits, _, _ = model(support_images, support_labels, query_images, n_way)\n",
        "\n",
        "    # Ensure logits are valid before loss calculation\n",
        "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "         print(\"Warning: NaN/Inf logits detected BEFORE loss calculation. Skipping batch.\")\n",
        "         return float('nan'), 0.0\n",
        "\n",
        "    loss = loss_fn(logits, query_labels)\n",
        "\n",
        "    # Check for NaN loss AFTER calculation\n",
        "    if torch.isnan(loss):\n",
        "        print(\"Warning: NaN loss calculated. Skipping backward pass for this batch.\")\n",
        "        return float('nan'), 0.0 # Return NaN loss and 0 acc\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient Clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_norm)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    # Use no_grad for accuracy calculation\n",
        "    with torch.no_grad():\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        accuracy = torch.mean((predictions == query_labels).float())\n",
        "\n",
        "    return loss.item(), accuracy.item()\n",
        "\n",
        "\n",
        "def evaluate_step(model, support_images, support_labels, query_images, query_labels, n_way):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "\n",
        "    support_images = support_images.to(DEVICE)\n",
        "    support_labels = support_labels.to(DEVICE)\n",
        "    query_images = query_images.to(DEVICE)\n",
        "    query_labels = query_labels.to(DEVICE)\n",
        "\n",
        "    with torch.no_grad(): # Ensure no gradients are computed\n",
        "        logits, _, _ = model(support_images, support_labels, query_images, n_way)\n",
        "\n",
        "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "            print(\"Warning: NaN/Inf detected in logits during evaluation. Returning 0 accuracy.\")\n",
        "            return 0.0\n",
        "\n",
        "    predictions = torch.argmax(logits, dim=1)\n",
        "    accuracy = torch.mean((predictions == query_labels).float())\n",
        "\n",
        "    return accuracy.item()\n",
        "\n",
        "def setup_optimizer(model, lr_backbone, lr_head, wd):\n",
        "     \"\"\" Creates optimizer with differential learning rates \"\"\"\n",
        "     backbone_params, head_params = model.encoder.get_trainable_parameters()\n",
        "     param_groups = [\n",
        "          {'params': backbone_params, 'lr': lr_backbone},\n",
        "          {'params': head_params, 'lr': lr_head}\n",
        "     ]\n",
        "     # Add other model parameters if they exist outside the encoder (currently none)\n",
        "     other_params = [p for n, p in model.named_parameters() if not n.startswith('encoder.') and p.requires_grad]\n",
        "     if other_params:\n",
        "          print(f\"Adding {len(other_params)} other parameters to optimizer (LR={lr_head}).\")\n",
        "          param_groups.append({'params': other_params, 'lr': lr_head})\n",
        "\n",
        "     optimizer = optim.AdamW(param_groups, weight_decay=wd)\n",
        "     return optimizer\n",
        "\n",
        "\n",
        "def main_training_loop(model, train_sampler, test_sampler, n_train_episodes, n_test_episodes,\n",
        "                      lr_backbone, lr_head, wd, label_smoothing, grad_clip_norm):\n",
        "    # Setup optimizer with differential LR\n",
        "    optimizer = setup_optimizer(model, lr_backbone, lr_head, wd)\n",
        "    # Use label smoothing loss\n",
        "    loss_fn = LabelSmoothingLoss(classes=train_sampler.n_way, smoothing=label_smoothing).to(DEVICE)\n",
        "\n",
        "    train_losses, train_accuracies, test_accuracies = [], [], []\n",
        "    best_test_acc = 0.0\n",
        "    test_eval_interval = 500 # How often to evaluate on test set\n",
        "    log_interval = 100\n",
        "\n",
        "    print(\"\\n--- Starting Meta-Training ---\")\n",
        "    print(f\"Config: N-Way={train_sampler.n_way}, K-Shot={train_sampler.k_shot}, Q={train_sampler.n_query}\")\n",
        "    print(f\"LRs: Backbone={lr_backbone}, Head={lr_head}, WD={wd}, LS={label_smoothing}\")\n",
        "    print(f\"Episodes: Train={n_train_episodes}, Test={n_test_episodes}, GradClip={grad_clip_norm}\")\n",
        "\n",
        "    pbar = tqdm(range(n_train_episodes))\n",
        "    for episode_idx in pbar:\n",
        "        try:\n",
        "            support_images, support_labels, query_images, query_labels = train_sampler.sample()\n",
        "        except ValueError as e:\n",
        "             print(f\"Error sampling train episode {episode_idx}: {e}. Skipping.\")\n",
        "             continue # Skip this episode\n",
        "\n",
        "        loss, acc = train_step(model, optimizer, loss_fn, support_images, support_labels,\n",
        "                              query_images, query_labels, train_sampler.n_way, grad_clip_norm)\n",
        "\n",
        "        if not math.isnan(loss):\n",
        "            train_losses.append(loss)\n",
        "            train_accuracies.append(acc)\n",
        "\n",
        "            if (episode_idx + 1) % log_interval == 0 and train_losses:\n",
        "                avg_loss = np.mean(train_losses[-log_interval:])\n",
        "                avg_acc = np.mean(train_accuracies[-log_interval:])\n",
        "                pbar.set_description(f\"Ep {episode_idx+1} | Loss: {avg_loss:.4f} | Acc: {avg_acc:.4f} | Best Test: {best_test_acc:.4f}\")\n",
        "\n",
        "            # Evaluate on test set periodically\n",
        "            if (episode_idx + 1) % test_eval_interval == 0:\n",
        "                print(f\"\\n--- Evaluating on Meta-Test set (Episode {episode_idx+1}) ---\")\n",
        "                try:\n",
        "                     current_test_acc = evaluate_on_test_set(model, test_sampler, n_test_episodes // 2) # Eval on fewer eps mid-train\n",
        "                     test_accuracies.append(current_test_acc)\n",
        "                     print(f\"Meta-Test Accuracy: {current_test_acc:.4f}\")\n",
        "\n",
        "                     if current_test_acc > best_test_acc:\n",
        "                         best_test_acc = current_test_acc\n",
        "                         print(f\"*** New best test accuracy: {best_test_acc:.4f}. Saving model... ***\")\n",
        "                         torch.save(model.state_dict(), \"best_cub_explainable_protonet.pth\")\n",
        "                except ValueError as e:\n",
        "                     print(f\"Skipping evaluation due to sampler error: {e}\")\n",
        "                # Restore train mode after eval\n",
        "                model.train()\n",
        "\n",
        "        else:\n",
        "             print(f\"Skipping logging for episode {episode_idx+1} due to NaN loss.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Finished Meta-Training ---\")\n",
        "    print(f\"Best Meta-Test Accuracy recorded: {best_test_acc:.4f}\")\n",
        "\n",
        "    # Final evaluation on more episodes\n",
        "    print(\"\\n--- Final Evaluation on Meta-Test set ---\")\n",
        "    try:\n",
        "        final_test_accuracy = evaluate_on_test_set(model, test_sampler, n_test_episodes)\n",
        "        print(f\"Final Meta-Test Accuracy ({n_test_episodes} episodes): {final_test_accuracy:.4f}\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Final evaluation failed due to sampler error: {e}\")\n",
        "         final_test_accuracy = 0.0\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Smooth curves for better visualization (simple moving average)\n",
        "    def smooth_curve(points, factor=0.9):\n",
        "        smoothed_points = []\n",
        "        for point in points:\n",
        "            if smoothed_points:\n",
        "                previous = smoothed_points[-1]\n",
        "                smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "            else:\n",
        "                smoothed_points.append(point)\n",
        "        return smoothed_points\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    if train_losses:\n",
        "        plt.plot(smooth_curve(train_losses), label='Smoothed Training Loss')\n",
        "        plt.title(\"Training Loss\")\n",
        "        plt.xlabel(\"Episode\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.ylim(bottom=0)\n",
        "        plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, \"No valid loss data recorded\", ha='center', va='center')\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if train_accuracies:\n",
        "        plt.plot(smooth_curve(train_accuracies), label=\"Smoothed Train Acc\")\n",
        "    if test_accuracies:\n",
        "       eval_points = np.linspace(test_eval_interval, n_train_episodes, len(test_accuracies))\n",
        "       plt.plot(eval_points - 1, test_accuracies, label=\"Test Acc\", marker='o', linestyle='--', markersize=5)\n",
        "\n",
        "    plt.title(\"Accuracy\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def evaluate_on_test_set(model, test_sampler, n_episodes):\n",
        "    model.eval()\n",
        "    all_accuracies = []\n",
        "    pbar = tqdm(range(n_episodes), desc=\"Meta-Testing\", leave=False)\n",
        "    for i in pbar:\n",
        "        try:\n",
        "             support_images, support_labels, query_images, query_labels = test_sampler.sample()\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping test episode {i+1} due to sampler error: {e}\")\n",
        "            continue # Skip if sampling fails\n",
        "\n",
        "        acc = evaluate_step(model, support_images, support_labels, query_images, query_labels, test_sampler.n_way)\n",
        "        if not math.isnan(acc):\n",
        "            all_accuracies.append(acc)\n",
        "            avg_acc = np.mean(all_accuracies) if all_accuracies else 0.0\n",
        "            pbar.set_description(f\"Meta-Testing (Avg Acc: {avg_acc:.4f})\")\n",
        "        else:\n",
        "             print(f\"Warning: NaN accuracy returned for test episode {i+1}.\")\n",
        "\n",
        "    if not all_accuracies:\n",
        "        print(\"Warning: No valid accuracies recorded during meta-test evaluation.\")\n",
        "        return 0.0\n",
        "    return np.mean(all_accuracies)"
      ],
      "metadata": {
        "id": "QLlFBGbQenvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Visualization & Explanation (Refined) ---\n",
        "# -------------------------------------\n",
        "inv_normalize_imagenet = transforms.Normalize(\n",
        "   mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "   std=[1/0.229, 1/0.224, 1/0.225]\n",
        ")\n",
        "\n",
        "def tensor_to_numpy_img(tensor_img):\n",
        "    \"\"\"Safely convert normalized tensor image to numpy array for display.\"\"\"\n",
        "    try:\n",
        "        img = tensor_img.detach().cpu() # Detach and move to CPU\n",
        "        img = inv_normalize_imagenet(img)\n",
        "        img = img.permute(1, 2, 0) # HWC\n",
        "        img = img.clamp(0, 1) # Ensure valid range\n",
        "        return img.numpy()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during tensor_to_numpy_img: {e}\")\n",
        "        return np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3)) # Return black image on error\n",
        "\n",
        "def map_patch_coords_to_image(flat_idx, patch_h, patch_w, total_stride, img_h=IMAGE_SIZE, img_w=IMAGE_SIZE, patch_vis_size=PATCH_SIZE_VIS):\n",
        "    \"\"\"Maps flat feature map patch index to image coordinates. Handles zero dimensions.\"\"\"\n",
        "    if patch_h <= 0 or patch_w <= 0:\n",
        "        # print(\"Warning: Zero spatial dimension in feature map for patch mapping.\")\n",
        "        return 0, 0 # Return top-left corner gracefully\n",
        "\n",
        "    patch_y = flat_idx // patch_w\n",
        "    patch_x = flat_idx % patch_w\n",
        "\n",
        "    # Calculate center of the receptive field corresponding to the patch center\n",
        "    center_y = (patch_y + 0.5) * total_stride\n",
        "    center_x = (patch_x + 0.5) * total_stride\n",
        "\n",
        "    # Calculate top-left corner of the visualization box\n",
        "    half_patch_vis = patch_vis_size // 2\n",
        "    top_left_y = max(0, int(center_y - half_patch_vis))\n",
        "    top_left_x = max(0, int(center_x - half_patch_vis))\n",
        "\n",
        "    # Ensure box doesn't go out of bounds\n",
        "    top_left_x = min(top_left_x, img_w - patch_vis_size)\n",
        "    top_left_y = min(top_left_y, img_h - patch_vis_size)\n",
        "\n",
        "    return top_left_x, top_left_y"
      ],
      "metadata": {
        "id": "dXYV4AQlet72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_explanation(model, test_sampler, num_explanations=1, top_k_vis=100):\n",
        "    \"\"\"\n",
        "    Visualizes Level 1 prediction and Level 2 Top-K patch explanations.\n",
        "    Highlights Top-K matches within each relevant support image.\n",
        "    Highlights the corresponding Top-K query patches overall.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Visualizing Top-{top_k_vis} Explanations for {num_explanations} Test Episodes ---\")\n",
        "    model.eval()\n",
        "    # Colors for Top-K visualization (Query and Support)\n",
        "    # Ensure enough colors if top_k_vis > 3\n",
        "    highlight_colors = ['red', 'orange', 'yellow', 'magenta', 'cyan']\n",
        "    support_highlight_colors = ['lime', 'greenyellow', 'palegreen', 'lightgreen', 'springgreen']\n",
        "\n",
        "\n",
        "    for vis_idx in range(num_explanations):\n",
        "        print(f\"\\n--- Explanation Example {vis_idx + 1}/{num_explanations} ---\")\n",
        "        try:\n",
        "            support_imgs, support_lbls, query_imgs, query_lbls = test_sampler.sample()\n",
        "        except ValueError as e:\n",
        "            print(f\"Skipping visualization {vis_idx+1} due to sampler error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Select a random query image from the episode to explain\n",
        "        if query_imgs.size(0) == 0:\n",
        "            print(\"Sampled query set is empty, cannot explain.\")\n",
        "            continue\n",
        "        query_idx_to_explain = random.randrange(query_imgs.size(0))\n",
        "\n",
        "        # --- Move data to device ---\n",
        "        support_images_dev = support_imgs.to(DEVICE)\n",
        "        support_labels_dev = support_lbls.to(DEVICE)\n",
        "        query_image_single_dev = query_imgs[query_idx_to_explain].unsqueeze(0).to(DEVICE)\n",
        "        true_label = query_lbls[query_idx_to_explain].item()\n",
        "\n",
        "        n_way = test_sampler.n_way\n",
        "        k_shot = test_sampler.k_shot\n",
        "\n",
        "        # --- Perform Inference (Level 1) ---\n",
        "        with torch.no_grad():\n",
        "            _, support_embeddings = model.encoder(support_images_dev)\n",
        "            query_conv_feature_single, query_embedding_single = model.encoder(query_image_single_dev)\n",
        "            support_conv_features_all, _ = model.encoder(support_images_dev)\n",
        "\n",
        "            prototypes = model._calculate_prototypes(support_embeddings, support_labels_dev, n_way)\n",
        "            distances = model._compute_distances(query_embedding_single, prototypes)\n",
        "            predicted_label = torch.argmin(distances, dim=1).item()\n",
        "\n",
        "        print(f\"Explaining Query Image Index: {query_idx_to_explain}\")\n",
        "        print(f\"Predicted Label (local 0-{n_way-1}): {predicted_label}\")\n",
        "        print(f\"True Label (local 0-{n_way-1}): {true_label}\")\n",
        "        print(f\"Correct Prediction: {'Yes' if predicted_label == true_label else 'No'}\")\n",
        "\n",
        "        # --- Level 2 Explanation (Get Top-K per support image) ---\n",
        "        predicted_class_support_indices = [i for i, lbl in enumerate(support_labels_dev) if lbl.item() == predicted_label]\n",
        "\n",
        "        if not predicted_class_support_indices:\n",
        "             print(f\"No support images found for predicted class {predicted_label}.\")\n",
        "             explanation_data_per_support = {}\n",
        "        else:\n",
        "            # Use the new Top-K explanation method\n",
        "             explanation_data_per_support = model.level2_explain_per_support_image_topk( # <--- Correctly used here\n",
        "                                                query_conv_feature_single,\n",
        "                                                support_conv_features_all,\n",
        "                                                predicted_class_support_indices,\n",
        "                                                top_k=top_k_vis)\n",
        "\n",
        "        if not explanation_data_per_support:\n",
        "            print(\"Could not generate detailed Top-K patch explanations.\")\n",
        "        else:\n",
        "            print(f\"Generated Top-{top_k_vis} patch explanations for {len(explanation_data_per_support)} relevant support image(s).\")\n",
        "\n",
        "\n",
        "        # --- Visualization ---\n",
        "        # Determine overall figure layout dynamically\n",
        "        num_relevant_support = len(predicted_class_support_indices)\n",
        "        # Rows needed: 1 for query + ceil(num_relevant_support / N_COLS)\n",
        "        N_COLS_VIS = min(max(k_shot, 3), 5) # Number of columns for support images (limit width)\n",
        "        N_ROWS_VIS = 1 + math.ceil(num_relevant_support / N_COLS_VIS) if num_relevant_support > 0 else 1\n",
        "        plt.figure(figsize=(3 * N_COLS_VIS, 3 * N_ROWS_VIS + 1 )) # Adjust figure size based on content\n",
        "        plt.suptitle(f\"Explanation {vis_idx+1}: Query {query_idx_to_explain} (Pred: {predicted_label}, True: {true_label}) | Top-{top_k_vis} Matches\", fontsize=14, y=0.99)\n",
        "\n",
        "\n",
        "        # == Display Query Image (Always in top-left) ==\n",
        "        ax_query = plt.subplot(N_ROWS_VIS, N_COLS_VIS, 1)\n",
        "        query_img_np = tensor_to_numpy_img(query_imgs[query_idx_to_explain])\n",
        "        ax_query.imshow(query_img_np)\n",
        "        ax_query.set_title(f\"Query Img {query_idx_to_explain}\")\n",
        "        ax_query.axis('off')\n",
        "\n",
        "        # == Highlight Top-K Overall Query Patches ==\n",
        "        # Collect all top matches across all support images to find overall top query patches\n",
        "        all_matches = []\n",
        "        for sup_idx, top_list in explanation_data_per_support.items():\n",
        "            for q_idx, s_idx, sim in top_list:\n",
        "                 all_matches.append({'q_idx': q_idx, 's_glob_idx': sup_idx, 's_loc_idx': s_idx, 'sim': sim})\n",
        "\n",
        "        # Sort all matches by similarity to find the absolute best ones\n",
        "        all_matches.sort(key=lambda x: x['sim'], reverse=True)\n",
        "\n",
        "        # Identify the top 'top_k_vis' *unique* query patches involved in the best matches\n",
        "        top_overall_query_indices_drawn = set()\n",
        "        highlights_on_query = [] # Store info for highlighting [(q_idx, color_idx), ...]\n",
        "        rank_counter = 0\n",
        "        for match in all_matches:\n",
        "            q_idx = match['q_idx']\n",
        "            if q_idx not in top_overall_query_indices_drawn:\n",
        "                 if rank_counter < top_k_vis:\n",
        "                    highlights_on_query.append((q_idx, rank_counter))\n",
        "                    top_overall_query_indices_drawn.add(q_idx)\n",
        "                    rank_counter += 1\n",
        "                 else:\n",
        "                      break # Stop after finding top_k_vis unique query patches\n",
        "\n",
        "        # Draw highlights on query image\n",
        "        B, D, H_feat_q, W_feat_q = query_conv_feature_single.shape\n",
        "        total_stride = model.encoder.total_stride if hasattr(model.encoder, 'total_stride') else 32 # Default stride\n",
        "        if H_feat_q > 0 and W_feat_q > 0:\n",
        "            for q_idx, color_idx in highlights_on_query:\n",
        "                q_patch_x, q_patch_y = map_patch_coords_to_image(q_idx, H_feat_q, W_feat_q, total_stride)\n",
        "                color = highlight_colors[color_idx % len(highlight_colors)]\n",
        "                q_rect = Rectangle((q_patch_x, q_patch_y), PATCH_SIZE_VIS, PATCH_SIZE_VIS,\n",
        "                                   linewidth=2.5, edgecolor=color, facecolor='none', linestyle='--')\n",
        "                ax_query.add_patch(q_rect)\n",
        "                ax_query.text(q_patch_x + 2, q_patch_y + 2, f\"M{color_idx+1}\", color=color, # Label as Match 1, 2, 3\n",
        "                              fontsize=9, fontweight='bold', ha='left', va='top',\n",
        "                              bbox=dict(facecolor='white', alpha=0.7, pad=0.1))\n",
        "\n",
        "        # == Display Relevant Support Images & Highlight Top-K Matches Within Each ==\n",
        "        subplot_offset = N_COLS_VIS # Start plotting support images from the second row\n",
        "\n",
        "        for plot_idx, support_global_idx in enumerate(predicted_class_support_indices):\n",
        "            subplot_index = subplot_offset + plot_idx + 1\n",
        "            if subplot_index > N_ROWS_VIS * N_COLS_VIS: # Avoid plotting beyond figure limits\n",
        "                 print(\"Warning: Too many support images for figure layout.\")\n",
        "                 break\n",
        "\n",
        "            ax_supp = plt.subplot(N_ROWS_VIS, N_COLS_VIS, subplot_index)\n",
        "            support_img_np = tensor_to_numpy_img(support_imgs[support_global_idx])\n",
        "            ax_supp.imshow(support_img_np)\n",
        "            ax_supp.set_title(f\"Support Img {support_global_idx}\\n(Pred Cls {predicted_label})\")\n",
        "            ax_supp.axis('off')\n",
        "\n",
        "            # Find the explanation entries for *this* specific support image\n",
        "            if support_global_idx in explanation_data_per_support:\n",
        "                top_k_matches_for_this = explanation_data_per_support[support_global_idx]\n",
        "\n",
        "                # Highlight the top-k support patches in *this* image\n",
        "                if support_conv_features_all.numel() > 0:\n",
        "                     _, _, H_feat_s, W_feat_s = support_conv_features_all[support_global_idx].unsqueeze(0).shape\n",
        "                     if H_feat_s > 0 and W_feat_s > 0:\n",
        "                        for rank, (q_idx, s_idx_in_img, sim) in enumerate(top_k_matches_for_this):\n",
        "                            if rank >= top_k_vis: break # Only show top_k_vis boxes\n",
        "                            s_patch_x, s_patch_y = map_patch_coords_to_image(s_idx_in_img, H_feat_s, W_feat_s, total_stride)\n",
        "                            # Use slightly different colors for support patches\n",
        "                            color = support_highlight_colors[rank % len(support_highlight_colors)]\n",
        "                            s_rect = Rectangle((s_patch_x, s_patch_y), PATCH_SIZE_VIS, PATCH_SIZE_VIS,\n",
        "                                               linewidth=2, edgecolor=color, facecolor='none')\n",
        "                            ax_supp.add_patch(s_rect)\n",
        "                            # Optional: Add text labels if needed, e.g., Rank number\n",
        "                            ax_supp.text(s_patch_x+1, s_patch_y+1, f\"{rank+1}\", color='black', backgroundcolor=color, alpha=0.8, fontsize=7, ha='left', va='top')\n",
        "                            # Find corresponding query highlight rank/color\n",
        "                            query_match_rank = -1\n",
        "                            for q_r, (q_highlight_idx, q_color_idx) in enumerate(highlights_on_query):\n",
        "                                 if q_highlight_idx == q_idx:\n",
        "                                      query_match_rank = q_r + 1\n",
        "                                      break\n",
        "                            # ax_supp.text(s_patch_x+1, s_patch_y+PATCH_SIZE_VIS-1, f\"-> Q{query_match_rank}\", color='black', fontsize=6)\n",
        "\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust rect to prevent title overlap\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "N4AfdQxteBkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------\n",
        "# --- Main Execution ---\n",
        "# -------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    SPLIT_FILE = \"cub_meta_split.json\" # Define the filename\n",
        "    FORCE_RESPLIT = True # Set to True if you want to ignore the file and make a new split\n",
        "\n",
        "    print(\"1. Preparing CUB Data Splits...\")\n",
        "    meta_train_data, meta_test_data, n_train_classes, n_test_classes = prepare_cub_data_splits(DATA_DIR)\n",
        "\n",
        "    if meta_train_data and meta_test_data:\n",
        "        print(\"\\n2. Creating Episode Samplers...\")\n",
        "        train_sampler = EpisodeSampler(meta_train_data, N_WAY, K_SHOT, N_QUERY)\n",
        "        test_sampler = EpisodeSampler(meta_test_data, N_WAY, K_SHOT, N_QUERY)\n",
        "\n",
        "        # Sample and print shapes to verify\n",
        "        s_img, s_lbl, q_img, q_lbl = train_sampler.sample()\n",
        "        print(\"Sampled Train Episode shapes:\")\n",
        "        print(f\"  Support: {s_img.shape}, {s_lbl.shape} | Query: {q_img.shape}, {q_lbl.shape}\")\n",
        "\n",
        "        print(\"\\n3. Initializing Model (Improved ResNet18)...\")\n",
        "        encoder = ResNetEncoderWithDropout(\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            pretrained=PRETRAINED,\n",
        "            freeze_until=FREEZE_UNTIL_LAYER,\n",
        "            dropout_rate=0.5 # Default, can be tuned\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        model = ExplainablePrototypicalNet(encoder).to(DEVICE)\n",
        "\n",
        "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"Model Parameters: Total={total_params}, Trainable={trainable_params} (after freezing)\")\n",
        "\n",
        "        print(\"\\n4. Starting Training...\")\n",
        "        # Pass new training hyperparameters\n",
        "        trained_model = main_training_loop(model, train_sampler, test_sampler,\n",
        "                                           n_train_episodes=N_TRAIN_EPISODES,\n",
        "                                           n_test_episodes=N_TEST_EPISODES,\n",
        "                                           lr_backbone=LR_BACKBONE,\n",
        "                                           lr_head=LR_HEAD,\n",
        "                                           wd=WEIGHT_DECAY,\n",
        "                                           label_smoothing=LABEL_SMOOTHING,\n",
        "                                           grad_clip_norm=GRADIENT_CLIP_NORM)\n",
        "\n",
        "        print(\"\\n5. Visualizing Explanation on Test Episodes...\")\n",
        "        visualize_explanation(trained_model, test_sampler, num_explanations=5)\n",
        "\n",
        "    else:\n",
        "        print(\"\\n--- ERROR: Failed to prepare data. Check paths and data integrity. Exiting. ---\")"
      ],
      "metadata": {
        "id": "JqozWYEge4B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_explanation(trained_model, test_sampler, num_explanations=10)"
      ],
      "metadata": {
        "id": "dwc1wEHRioMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gnc329CpvrU_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}